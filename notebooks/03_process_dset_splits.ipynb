{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s') # NOTSET, DEBUG, INFO, WARN, ERROR, CRITICAL\n",
    "\n",
    "from JPAS_DA.data import loading_tools\n",
    "from JPAS_DA.data import cleaning_tools\n",
    "from JPAS_DA.data import crossmatch_tools\n",
    "from JPAS_DA.data import process_dset_splits\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from JPAS_DA.utils import plotting_utils\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('default')\n",
    "plt.close('all')\n",
    "font, rcnew = plotting_utils.matplotlib_default_config()\n",
    "mpl.rc('font', **font)\n",
    "plt.rcParams.update(rcnew)\n",
    "plt.style.use('tableau-colorblind10')\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = \"/home/dlopez/Documents/Projects/JPAS_Domain_Adaptation/DATA/noise_jpas_v1/Train-Validate-Test\"\n",
    "\n",
    "load_JPAS_data = [{\n",
    "    \"name\": \"all\",\n",
    "    \"npy\": \"JPAS_DATA_Aper_Cor_3_FLUX+NOISE.npy\",\n",
    "    \"csv\": \"JPAS_DATA_PROPERTIES.csv\",\n",
    "    \"sample_percentage\": 1.0  # Optional, defaults to 1.0\n",
    "}]\n",
    "\n",
    "load_DESI_data = [\n",
    "{\n",
    "    \"name\": \"train\",\n",
    "    \"npy\": \"mock_3_train.npy\",\n",
    "    \"csv\": \"props_training.csv\",\n",
    "    \"sample_percentage\": 0.3\n",
    "},\n",
    "{\n",
    "    \"name\": \"val\",\n",
    "    \"npy\": \"mock_3_validate.npy\",\n",
    "    \"csv\": \"props_validate.csv\",\n",
    "    \"sample_percentage\": 1.0\n",
    "},\n",
    "{\n",
    "    \"name\": \"test\",\n",
    "    \"npy\": \"mock_3_test.npy\",\n",
    "    \"csv\": \"props_test.csv\",\n",
    "    \"sample_percentage\": 1.0\n",
    "}\n",
    "]\n",
    "\n",
    "random_seed_load = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-16 12:49:23,693 - INFO - ðŸ“¥ Starting full dataset loading with `load_dsets()`\n",
      "2025-05-16 12:49:23,693 - INFO - â”œ Loading JPAS datasets...\n",
      "2025-05-16 12:49:23,694 - INFO - â”œâ”€â”€â”€ ðŸ“¥ Starting JPAS dataset loading...\n",
      "2025-05-16 12:49:23,694 - INFO - |    â”œâ”€â”€â”€ ðŸ”¹ Dataset: all (sample 100%)\n",
      "2025-05-16 12:49:23,748 - INFO - |    |    âœ” CSV loaded: JPAS_DATA_PROPERTIES.csv (shape: (52020, 18))\n",
      "2025-05-16 12:49:23,764 - INFO - |    |    âœ” NPY loaded: JPAS_DATA_Aper_Cor_3_FLUX+NOISE.npy (obs shape: (52020, 57))\n",
      "2025-05-16 12:49:23,765 - INFO - â”œâ”€â”€â”€ âœ… Finished loading all JPAS datasets.\n",
      "2025-05-16 12:49:23,766 - INFO - â”œ Loading DESI datasets (splitted)...\n",
      "2025-05-16 12:49:23,766 - INFO - â”œâ”€â”€â”€ ðŸ“¥ Starting DESI dataset loading...\n",
      "2025-05-16 12:49:23,766 - INFO - |    â”œâ”€â”€â”€ ðŸ”¹ Dataset: train\n",
      "2025-05-16 12:49:24,716 - INFO - |    |    âœ” CSV loaded ((1087882, 18)), Size: 445.74 MB\n",
      "2025-05-16 12:49:24,717 - INFO - |    |    âœ” NPY loaded ((1087882, 57, 3)), Size: 1488.22 MB\n",
      "2025-05-16 12:49:24,724 - INFO - |    |    ðŸ“‰ Sampling 326364/1087882 rows (30%)\n",
      "2025-05-16 12:49:24,919 - INFO - |    |    âœ” Sample loaded. Final shape: (326364, 57, 3)\n",
      "2025-05-16 12:49:24,920 - INFO - |    â”œâ”€â”€â”€ ðŸ”¹ Dataset: val\n",
      "2025-05-16 12:49:25,125 - INFO - |    |    âœ” CSV loaded ((233118, 18)), Size: 95.51 MB\n",
      "2025-05-16 12:49:25,136 - INFO - |    |    âœ” NPY loaded ((233118, 57, 3)), Size: 318.91 MB\n",
      "2025-05-16 12:49:25,250 - INFO - |    |    âœ” Sample loaded. Final shape: (233118, 57, 3)\n",
      "2025-05-16 12:49:25,251 - INFO - |    â”œâ”€â”€â”€ ðŸ”¹ Dataset: test\n",
      "2025-05-16 12:49:25,446 - INFO - |    |    âœ” CSV loaded ((233118, 18)), Size: 95.52 MB\n",
      "2025-05-16 12:49:25,449 - INFO - |    |    âœ” NPY loaded ((233118, 57, 3)), Size: 318.91 MB\n",
      "2025-05-16 12:49:25,566 - INFO - |    |    âœ” Sample loaded. Final shape: (233118, 57, 3)\n",
      "2025-05-16 12:49:25,566 - INFO - â”œâ”€â”€â”€ âœ… Finished loading all DESI datasets.\n",
      "2025-05-16 12:49:25,572 - INFO - â”œ Concatenating DESI datasets...\n",
      "2025-05-16 12:49:25,572 - INFO - â”œâ”€â”€â”€ ðŸ”„ Concatenating DESI dataset splits...\n",
      "2025-05-16 12:49:25,572 - INFO - |    |    Identified split names: ['train', 'val', 'test']\n",
      "2025-05-16 12:49:25,733 - INFO - |    |    Merged NPY arrays into 'all_np' with shape (792600, 57, 3)\n",
      "2025-05-16 12:49:25,733 - INFO - â”œâ”€â”€â”€ âœ… DESI split concatenation complete.\n",
      "2025-05-16 12:49:25,733 - INFO - âœ… Finished `load_dsets()`\n"
     ]
    }
   ],
   "source": [
    "DATA = loading_tools.load_dsets(root_path=root_path, datasets_jpas=load_JPAS_data, datasets_desi=load_DESI_data, random_seed=random_seed_load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_clean_data_options = {\n",
    "    \"apply_masks\"         : [\"unreliable\", \"magic_numbers\", \"negative_errors\", \"nan_values\", \"apply_additional_filters\"],\n",
    "    \"mask_indices\"        : [0, -2],\n",
    "    \"magic_numbers\"       : [-99, 99],\n",
    "    \"i_band_sn_threshold\" : 0,\n",
    "    \"z_lim_QSO_cut\"       : 2.2\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-16 12:49:25,742 - INFO - ðŸ§½ Cleaning and masking data...\n",
      "2025-05-16 12:49:25,742 - INFO - â”œâ”€â”€ remove_invalid_NaN_rows()\n",
      "2025-05-16 12:49:25,930 - INFO - â”‚   â”œâ”€â”€ # objects filled with NaNs in JPAS: 0(0.0%)\n",
      "2025-05-16 12:49:25,930 - INFO - â”‚   â”œâ”€â”€ # objects filled with NaNs in DESI: 505(0.06%)\n",
      "2025-05-16 12:49:26,341 - INFO - â”œâ”€â”€ ðŸ§¹ Deleted cleaned DATA_clean dictionary to free memory.\n",
      "2025-05-16 12:49:26,342 - INFO - â”œâ”€â”€ apply_additional_filters()\n",
      "2025-05-16 12:49:26,351 - INFO - â”‚   â”œâ”€â”€ JPAS: 52020 valid rows (S/N â‰¥ 0) (100.0%)\n",
      "2025-05-16 12:49:26,351 - INFO - â”‚   â”œâ”€â”€ DESI: 792095 valid rows (S/N â‰¥ 0) (100.0%)\n",
      "2025-05-16 12:49:26,602 - INFO - â”‚   â”œâ”€â”€ Additional filters applied successfully.\n",
      "2025-05-16 12:49:26,604 - INFO - â”œâ”€â”€ Masking out indices [0, -2] (unreliable in DESI).\n",
      "2025-05-16 12:49:27,078 - INFO - â”‚   â”œâ”€â”€ Updated JPAS obs/err shape: (52020, 55)\n",
      "2025-05-16 12:49:27,078 - INFO - â”‚   â”œâ”€â”€ Updated DESI mean/err shape: (792095, 55)\n",
      "2025-05-16 12:49:27,079 - INFO - â”œâ”€â”€ Checking for magic numbers (99 and -99) in datasets.\n",
      "2025-05-16 12:49:27,128 - INFO - â”‚   â”œâ”€â”€ # objects containing some -99 entry in JPAS: 0(0.0%)\n",
      "2025-05-16 12:49:27,131 - INFO - â”‚   â”œâ”€â”€ # objects containing some 99 entry in JPAS: 0(0.0%)\n",
      "2025-05-16 12:49:27,163 - INFO - â”‚   â”œâ”€â”€ # objects containing some -99 entry in DESI: 99(0.01%)\n",
      "2025-05-16 12:49:27,196 - INFO - â”‚   â”œâ”€â”€ # objects containing some 99 entry in DESI: 0(0.0%)\n",
      "2025-05-16 12:49:27,216 - INFO - â”œâ”€â”€ Checking for negative errors in datasets.\n",
      "2025-05-16 12:49:27,241 - INFO - â”‚   â”œâ”€â”€ # objects containing some negative error entry in JPAS: 21521(41.37%)\n",
      "2025-05-16 12:49:27,274 - INFO - â”‚   â”œâ”€â”€ # objects containing some negative error entry in DESI: 0(0.0%)\n",
      "2025-05-16 12:49:27,324 - INFO - â”œâ”€â”€ Splitting between High and Low z QSOs\n",
      "2025-05-16 12:49:28,048 - INFO - â”œâ”€â”€ ðŸ”‘ Starting encoding process for string list...\n",
      "2025-05-16 12:49:28,248 - INFO - |    â”œâ”€â”€ ðŸ“Œ New Mapping Created: {np.str_('GALAXY'): 0, np.str_('QSO_high'): 1, np.str_('QSO_low'): 2, np.str_('STAR'): 3}\n",
      "2025-05-16 12:49:28,249 - INFO - â”œâ”€â”€ Encoding complete (4 categories).\n",
      "2025-05-16 12:49:28,249 - INFO - â”œâ”€â”€ ðŸ”‘ Starting encoding process for string list...\n",
      "2025-05-16 12:49:28,402 - INFO - |    â”œâ”€â”€ ðŸ“Œ Used Provided Mapping: {np.str_('GALAXY'): 0, np.str_('QSO_high'): 1, np.str_('QSO_low'): 2, np.str_('STAR'): 3}\n",
      "2025-05-16 12:49:28,403 - INFO - â”œâ”€â”€ Encoding complete (4 categories).\n",
      "2025-05-16 12:49:28,403 - INFO - â”œâ”€â”€ ðŸ”‘ Starting encoding process for string list...\n",
      "2025-05-16 12:49:28,413 - INFO - |    â”œâ”€â”€ ðŸ“Œ Used Provided Mapping: {np.str_('GALAXY'): 0, np.str_('QSO_high'): 1, np.str_('QSO_low'): 2, np.str_('STAR'): 3}\n",
      "2025-05-16 12:49:28,413 - INFO - â”œâ”€â”€ Encoding complete (4 categories).\n",
      "2025-05-16 12:49:28,416 - INFO - â”œâ”€â”€ ðŸ”‘ Starting encoding process for string list...\n",
      "2025-05-16 12:49:28,602 - INFO - |    â”œâ”€â”€ ðŸ“Œ New Mapping Created: {np.str_('DEV'): 0, np.str_('EXP'): 1, np.str_('GGAL'): 2, np.str_('GPSF'): 3, np.str_('PSF'): 4, np.str_('REX'): 5, np.str_('SER'): 6, np.str_('nan'): 7}\n",
      "2025-05-16 12:49:28,602 - INFO - â”œâ”€â”€ Encoding complete (8 categories).\n",
      "2025-05-16 12:49:28,603 - INFO - â”œâ”€â”€ ðŸ”‘ Starting encoding process for string list...\n",
      "2025-05-16 12:49:28,754 - INFO - |    â”œâ”€â”€ ðŸ“Œ Used Provided Mapping: {np.str_('DEV'): 0, np.str_('EXP'): 1, np.str_('GGAL'): 2, np.str_('GPSF'): 3, np.str_('PSF'): 4, np.str_('REX'): 5, np.str_('SER'): 6, np.str_('nan'): 7}\n",
      "2025-05-16 12:49:28,754 - INFO - â”œâ”€â”€ Encoding complete (8 categories).\n",
      "2025-05-16 12:49:28,755 - INFO - â”œâ”€â”€ ðŸ”‘ Starting encoding process for string list...\n",
      "2025-05-16 12:49:28,765 - INFO - |    â”œâ”€â”€ ðŸ“Œ Used Provided Mapping: {np.str_('DEV'): 0, np.str_('EXP'): 1, np.str_('GGAL'): 2, np.str_('GPSF'): 3, np.str_('PSF'): 4, np.str_('REX'): 5, np.str_('SER'): 6, np.str_('nan'): 7}\n",
      "2025-05-16 12:49:28,765 - INFO - â”œâ”€â”€ Encoding complete (8 categories).\n",
      "2025-05-16 12:49:28,766 - INFO - âœ… Finished clean_and_mask_data()\n"
     ]
    }
   ],
   "source": [
    "DATA = cleaning_tools.clean_and_mask_data(\n",
    "    DATA=DATA,\n",
    "    apply_masks=dict_clean_data_options[\"apply_masks\"],\n",
    "    mask_indices=dict_clean_data_options[\"mask_indices\"],\n",
    "    magic_numbers=dict_clean_data_options[\"magic_numbers\"],\n",
    "    i_band_sn_threshold=dict_clean_data_options[\"i_band_sn_threshold\"],\n",
    "    z_lim_QSO_cut=dict_clean_data_options[\"z_lim_QSO_cut\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-16 12:49:28,777 - INFO - ðŸ” crossmatch_IDs_two_datasets()...\n",
      "2025-05-16 12:49:28,778 - INFO - â”œâ”€â”€ ðŸš€ Starting ID categorization process...\n",
      "2025-05-16 12:49:28,787 - INFO - |    â”œâ”€â”€ ðŸ“Œ Found 804570 unique IDs across 2 arrays.\n",
      "2025-05-16 12:49:28,939 - INFO - |    â”œâ”€â”€ Presence matrix created with shape: (2, 804570)\n",
      "2025-05-16 12:49:28,940 - INFO - |    â”œâ”€â”€ Category mask created with shape: (2, 804570)\n",
      "2025-05-16 12:49:28,941 - INFO - â”œâ”€â”€ ðŸš€ Starting index retrieval process...\n",
      "2025-05-16 12:49:28,941 - INFO - |    â”œâ”€â”€ ðŸ“Œ Processing 804570 unique IDs across 2 arrays.\n",
      "2025-05-16 12:49:29,230 - INFO - â”œâ”€â”€ ðŸš€ Starting post-processing of unique IDs across two arrays...\n",
      "2025-05-16 12:49:29,245 - INFO - |    â”œâ”€â”€ Processing complete: 752550 IDs only in Array 1 (93.53%).\n",
      "2025-05-16 12:49:29,245 - INFO - |    â”œâ”€â”€ Processing complete: 24788 IDs only in Array 2 (3.08%).\n",
      "2025-05-16 12:49:29,246 - INFO - |    â”œâ”€â”€ Processing complete: 27232 IDs in both arrays (3.38%).\n",
      "2025-05-16 12:49:29,246 - INFO - âœ… Finished crossmatch_IDs_two_datasets()\n"
     ]
    }
   ],
   "source": [
    "Dict_LoA = {\"both\":{}, \"only\":{}} # Dictionary of Lists of Arrays (LoA) indicating, for each TARGETID, the associatted entries in the arrays, e.g. TARGETID[LoA[ii][0]] == TARGETID[LoA[ii][-1]]\n",
    "(\n",
    "    IDs_only_DESI, IDs_only_JPAS, IDs_both,\n",
    "    Dict_LoA[\"only\"][\"DESI\"], Dict_LoA[\"only\"][\"JPAS\"],\n",
    "    Dict_LoA[\"both\"][\"DESI\"], Dict_LoA[\"both\"][\"JPAS\"]\n",
    ") = crossmatch_tools.crossmatch_IDs_two_datasets(\n",
    "    DATA[\"DESI\"]['TARGETID'], DATA[\"JPAS\"]['TARGETID']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_split_data_options = {\n",
    "    \"train_ratio_both\"             : 0.8,\n",
    "    \"val_ratio_both\"               : 0.1,\n",
    "    \"test_ratio_both\"              : 0.1,\n",
    "    \"random_seed_split_both\"       : 42,\n",
    "    \"train_ratio_only_DESI\"        : 0.8,\n",
    "    \"val_ratio_only_DESI\"          : 0.1,\n",
    "    \"test_ratio_only_DESI\"         : 0.1,\n",
    "    \"random_seed_split_only_DESI\"  : 42\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-16 12:49:29,280 - INFO - â”œâ”€â”€ âœ‚ï¸ Splitting list of arrays (LoA) into train/val/test subsets...\n",
      "2025-05-16 12:49:29,284 - INFO - â”œâ”€â”€ Finished splitting.\n",
      "2025-05-16 12:49:29,285 - INFO - â”œâ”€â”€ âœ‚ï¸ Splitting list of arrays (LoA) into train/val/test subsets...\n",
      "2025-05-16 12:49:29,289 - INFO - â”œâ”€â”€ Finished splitting.\n",
      "2025-05-16 12:49:29,289 - INFO - â”œâ”€â”€ âœ‚ï¸ Splitting list of arrays (LoA) into train/val/test subsets...\n",
      "2025-05-16 12:49:29,408 - INFO - â”œâ”€â”€ Finished splitting.\n"
     ]
    }
   ],
   "source": [
    "# Split the Lists of Arrays into training, validation, and testing sets\n",
    "Dict_LoA_split = {\"both\":{}, \"only\":{}}\n",
    "Dict_LoA_split[\"both\"][\"JPAS\"] = process_dset_splits.split_LoA(\n",
    "    Dict_LoA[\"both\"][\"JPAS\"],\n",
    "    train_ratio = dict_split_data_options[\"train_ratio_both\"],\n",
    "    val_ratio = dict_split_data_options[\"val_ratio_both\"],\n",
    "    test_ratio = dict_split_data_options[\"test_ratio_both\"],\n",
    "    seed = dict_split_data_options[\"random_seed_split_both\"]\n",
    ")\n",
    "Dict_LoA_split[\"both\"][\"DESI\"] = process_dset_splits.split_LoA(\n",
    "    Dict_LoA[\"both\"][\"DESI\"],\n",
    "    train_ratio = dict_split_data_options[\"train_ratio_both\"],\n",
    "    val_ratio = dict_split_data_options[\"val_ratio_both\"],\n",
    "    test_ratio = dict_split_data_options[\"test_ratio_both\"],\n",
    "    seed = dict_split_data_options[\"random_seed_split_both\"]\n",
    ")\n",
    "Dict_LoA_split[\"only\"][\"DESI\"]  = process_dset_splits.split_LoA(\n",
    "    Dict_LoA[\"only\"][\"DESI\"],\n",
    "    train_ratio = dict_split_data_options[\"train_ratio_only_DESI\"],\n",
    "    val_ratio = dict_split_data_options[\"val_ratio_only_DESI\"],\n",
    "    test_ratio = dict_split_data_options[\"test_ratio_only_DESI\"],\n",
    "    seed = dict_split_data_options[\"random_seed_split_only_DESI\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TARGETID JPAS: 39633293729072001 TARGETID DESI: 39633293729072001\n",
      "TARGETID JPAS: 39633275194443316 TARGETID DESI: 39633275194443316\n",
      "TARGETID JPAS: 39633297344563253 TARGETID DESI: 39633297344563253\n",
      "TARGETID JPAS: 39633290071638383 TARGETID DESI: 39633290071638383\n",
      "TARGETID JPAS: 39633290067445314 TARGETID DESI: 39633290067445314\n",
      "TARGETID JPAS: 39633282656112085 TARGETID DESI: 39633282656112085\n",
      "TARGETID JPAS: 39633267661475062 TARGETID DESI: 39633267661475062\n",
      "TARGETID JPAS: 39633293712296354 TARGETID DESI: 39633293712296354\n",
      "TARGETID JPAS: 39633286355488472 TARGETID DESI: 39633286355488472\n",
      "TARGETID JPAS: 39633290042279158 TARGETID DESI: 39633290042279158\n",
      "TARGETID JPAS: 39633282689666769 TARGETID DESI: 39633282689666769\n",
      "TARGETID JPAS: 39633290046475536 TARGETID DESI: 39633290046475536\n",
      "TARGETID JPAS: 39633293691323183 TARGETID DESI: 39633293691323183\n",
      "TARGETID JPAS: 39633286376457480 TARGETID DESI: 39633286376457480\n",
      "TARGETID JPAS: 39633278965122779 TARGETID DESI: 39633278965122779\n"
     ]
    }
   ],
   "source": [
    "for ii, key_dset in enumerate(Dict_LoA_split[\"both\"][\"JPAS\"].keys()):\n",
    "    assert len(Dict_LoA_split[\"both\"][\"JPAS\"][key_dset]) == len(Dict_LoA_split[\"both\"][\"DESI\"][key_dset]), \"Both datasets must have the same number unique TARGETIDs in each of training, validation, and testing sets.\"\n",
    "    for jj in range(len(Dict_LoA_split[\"both\"][\"JPAS\"][key_dset])):\n",
    "        idx_ = Dict_LoA_split[\"both\"][\"JPAS\"][key_dset][jj][0]\n",
    "        tmp_TARGETID = DATA[\"JPAS\"][\"TARGETID\"][idx_]\n",
    "        for kk in range(len(Dict_LoA_split[\"both\"][\"DESI\"][key_dset][jj])):\n",
    "            idx_ = Dict_LoA_split[\"both\"][\"DESI\"][key_dset][jj][kk]\n",
    "            tmp_TARGETID_ = DATA[\"DESI\"][\"TARGETID\"][idx_]\n",
    "            assert tmp_TARGETID == tmp_TARGETID_, \"Both datasets must have the same TARGETIDs in each of training, validation, and testing sets.\"\n",
    "        if len(Dict_LoA_split[\"both\"][\"DESI\"][key_dset][jj]) > 2:\n",
    "            print(\"TARGETID JPAS:\", tmp_TARGETID, \"TARGETID DESI:\", tmp_TARGETID_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys_xx = ['OBS', 'ERR', 'MORPHTYPE_int']\n",
    "keys_yy = ['SPECTYPE_int', 'TARGETID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-16 12:49:29,447 - INFO - |    â”œâ”€â”€ ðŸ”§ extract_and_combine_DESI_data()\n",
      "2025-05-16 12:49:29,447 - INFO - |    â”œâ”€â”€ Extracting features and labels from DESI-only subset...\n",
      "2025-05-16 12:49:31,201 - INFO - |    â”œâ”€â”€ Extracting features and labels from DESI-matched subset...\n",
      "2025-05-16 12:49:31,365 - INFO - |    â”œâ”€â”€ Applied index shift of 611345 to matched DESI group to ensure uniqueness\n",
      "2025-05-16 12:49:31,436 - INFO - |    â”œâ”€â”€ Finished extract_and_combine_DESI_data()\n",
      "2025-05-16 12:49:31,440 - INFO - |    â”œâ”€â”€ ðŸ”§ extract_data_matched()\n",
      "2025-05-16 12:49:31,441 - INFO - |    â”œâ”€â”€ Extracting features and labels from matched dataset...\n",
      "2025-05-16 12:49:31,496 - INFO - |    â”œâ”€â”€ Finished extract_data_matched()\n",
      "2025-05-16 12:49:31,497 - INFO - |    â”œâ”€â”€ ðŸ”§ extract_data_matched()\n",
      "2025-05-16 12:49:31,497 - INFO - |    â”œâ”€â”€ Extracting features and labels from matched dataset...\n",
      "2025-05-16 12:49:31,531 - INFO - |    â”œâ”€â”€ Finished extract_data_matched()\n"
     ]
    }
   ],
   "source": [
    "key_dset = \"train\"\n",
    "\n",
    "LoA_combined, xx_combined, yy_combined = process_dset_splits.extract_and_combine_DESI_data(\n",
    "    Dict_LoA_split[\"only\"][\"DESI\"][key_dset], Dict_LoA_split[\"both\"][\"DESI\"][key_dset], DATA[\"DESI\"], keys_xx, keys_yy\n",
    ")\n",
    "LoA_DESI, xx_DESI, yy_DESI = process_dset_splits.extract_data_matched(Dict_LoA_split[\"both\"][\"DESI\"][key_dset], DATA[\"DESI\"], keys_xx, keys_yy)\n",
    "LoA_JPAS, xx_JPAS, yy_JPAS = process_dset_splits.extract_data_matched(Dict_LoA_split[\"both\"][\"JPAS\"][key_dset], DATA[\"JPAS\"], keys_xx, keys_yy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "623825\n",
      "(633698,)\n",
      "dict_keys(['OBS', 'ERR', 'MORPHTYPE_int'])\n",
      "633698\n",
      "(633698, 55)\n",
      "633698\n",
      "(633698,)\n",
      "dict_keys(['SPECTYPE_int', 'TARGETID'])\n",
      "633698\n",
      "(633698,)\n",
      "633698\n",
      "(633698,)\n"
     ]
    }
   ],
   "source": [
    "print(len(LoA_combined))\n",
    "print(np.concatenate(LoA_combined).shape)\n",
    "\n",
    "print(xx_combined.keys())\n",
    "print(len(xx_combined['OBS']))\n",
    "print(xx_combined['OBS'].shape)\n",
    "print(len(xx_combined['MORPHTYPE_int']))\n",
    "print(xx_combined['MORPHTYPE_int'].shape)\n",
    "\n",
    "print(yy_combined.keys())\n",
    "print(len(yy_combined['SPECTYPE_int']))\n",
    "print(yy_combined['SPECTYPE_int'].shape)\n",
    "print(len(yy_combined['TARGETID']))\n",
    "print(yy_combined['TARGETID'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21785\n",
      "(21785,)\n",
      "dict_keys(['OBS', 'ERR', 'MORPHTYPE_int'])\n",
      "21785\n",
      "(21785, 55)\n",
      "21785\n",
      "(21785,)\n",
      "dict_keys(['SPECTYPE_int', 'TARGETID'])\n",
      "21785\n",
      "(21785,)\n",
      "21785\n",
      "(21785,)\n"
     ]
    }
   ],
   "source": [
    "print(len(LoA_JPAS))\n",
    "print(np.concatenate(LoA_JPAS).shape)\n",
    "\n",
    "print(xx_JPAS.keys())\n",
    "print(len(xx_JPAS['OBS']))\n",
    "print(xx_JPAS['OBS'].shape)\n",
    "print(len(xx_JPAS['MORPHTYPE_int']))\n",
    "print(xx_JPAS['MORPHTYPE_int'].shape)\n",
    "\n",
    "print(yy_JPAS.keys())\n",
    "print(len(yy_JPAS['SPECTYPE_int']))\n",
    "print(yy_JPAS['SPECTYPE_int'].shape)\n",
    "print(len(yy_JPAS['TARGETID']))\n",
    "print(yy_JPAS['TARGETID'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JPAS TARGETID 39633132139316722\n",
      "DESI TARGETID 39633132139316722\n",
      "\n",
      "JPAS TARGETID 39633290067446186\n",
      "DESI TARGETID 39633290067446186\n",
      "\n",
      "JPAS TARGETID 39633136572695155\n",
      "DESI TARGETID 39633136572695155\n",
      "\n",
      "JPAS TARGETID 39633282664499214\n",
      "DESI TARGETID 39633282664499214\n",
      "DESI TARGETID 39633282664499214\n",
      "\n",
      "JPAS TARGETID 39633132118346665\n",
      "DESI TARGETID 39633132118346665\n",
      "\n"
     ]
    }
   ],
   "source": [
    "assert len(LoA_DESI) == len(LoA_JPAS), \"Both datasets must have the same number unique TARGETIDs\"\n",
    "assert np.unique(yy_JPAS['TARGETID']).shape == np.unique(yy_DESI['TARGETID']).shape, \"Both datasets must have the same number unique TARGETIDs\"\n",
    "\n",
    "for ii in np.arange(45,50):\n",
    "    for jj in range(len(LoA_JPAS[ii])):\n",
    "        print(\"JPAS TARGETID\", yy_JPAS['TARGETID'][LoA_JPAS[ii][jj]])\n",
    "    for jj in range(len(LoA_DESI[ii])):\n",
    "        print(\"DESI TARGETID\", yy_DESI['TARGETID'][LoA_DESI[ii][jj]])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
