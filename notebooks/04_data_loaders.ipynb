{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s') # NOTSET, DEBUG, INFO, WARN, ERROR, CRITICAL\n",
    "\n",
    "from JPAS_DA.data import loading_tools\n",
    "from JPAS_DA.data import cleaning_tools\n",
    "from JPAS_DA.data import crossmatch_tools\n",
    "from JPAS_DA.data import process_dset_splits\n",
    "from JPAS_DA.data import data_loaders\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from JPAS_DA.utils import plotting_utils\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('default')\n",
    "plt.close('all')\n",
    "font, rcnew = plotting_utils.matplotlib_default_config()\n",
    "mpl.rc('font', **font)\n",
    "plt.rcParams.update(rcnew)\n",
    "plt.style.use('tableau-colorblind10')\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = \"/home/dlopez/Documents/Projects/JPAS_Domain_Adaptation/DATA/noise_jpas_v1/Train-Validate-Test\"\n",
    "\n",
    "load_JPAS_data = [{\n",
    "    \"name\": \"all\",\n",
    "    \"npy\": \"JPAS_DATA_Aper_Cor_3_FLUX+NOISE.npy\",\n",
    "    \"csv\": \"JPAS_DATA_PROPERTIES.csv\",\n",
    "    \"sample_percentage\": 1.0  # Optional, defaults to 1.0\n",
    "}]\n",
    "\n",
    "load_DESI_data = [\n",
    "{\n",
    "    \"name\": \"train\",\n",
    "    \"npy\": \"mock_3_train.npy\",\n",
    "    \"csv\": \"props_training.csv\",\n",
    "    \"sample_percentage\": 0.3\n",
    "},\n",
    "{\n",
    "    \"name\": \"val\",\n",
    "    \"npy\": \"mock_3_validate.npy\",\n",
    "    \"csv\": \"props_validate.csv\",\n",
    "    \"sample_percentage\": 1.0\n",
    "},\n",
    "{\n",
    "    \"name\": \"test\",\n",
    "    \"npy\": \"mock_3_test.npy\",\n",
    "    \"csv\": \"props_test.csv\",\n",
    "    \"sample_percentage\": 1.0\n",
    "}\n",
    "]\n",
    "\n",
    "random_seed_load = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-16 12:50:49,821 - INFO - 📥 Starting full dataset loading with `load_dsets()`\n",
      "2025-05-16 12:50:49,821 - INFO - ├ Loading JPAS datasets...\n",
      "2025-05-16 12:50:49,821 - INFO - ├─── 📥 Starting JPAS dataset loading...\n",
      "2025-05-16 12:50:49,821 - INFO - |    ├─── 🔹 Dataset: all (sample 100%)\n",
      "2025-05-16 12:50:49,863 - INFO - |    |    ✔ CSV loaded: JPAS_DATA_PROPERTIES.csv (shape: (52020, 18))\n",
      "2025-05-16 12:50:49,874 - INFO - |    |    ✔ NPY loaded: JPAS_DATA_Aper_Cor_3_FLUX+NOISE.npy (obs shape: (52020, 57))\n",
      "2025-05-16 12:50:49,874 - INFO - ├─── ✅ Finished loading all JPAS datasets.\n",
      "2025-05-16 12:50:49,875 - INFO - ├ Loading DESI datasets (splitted)...\n",
      "2025-05-16 12:50:49,875 - INFO - ├─── 📥 Starting DESI dataset loading...\n",
      "2025-05-16 12:50:49,876 - INFO - |    ├─── 🔹 Dataset: train\n",
      "2025-05-16 12:50:50,796 - INFO - |    |    ✔ CSV loaded ((1087882, 18)), Size: 445.74 MB\n",
      "2025-05-16 12:50:50,797 - INFO - |    |    ✔ NPY loaded ((1087882, 57, 3)), Size: 1488.22 MB\n",
      "2025-05-16 12:50:50,803 - INFO - |    |    📉 Sampling 326364/1087882 rows (30%)\n",
      "2025-05-16 12:50:50,997 - INFO - |    |    ✔ Sample loaded. Final shape: (326364, 57, 3)\n",
      "2025-05-16 12:50:50,997 - INFO - |    ├─── 🔹 Dataset: val\n",
      "2025-05-16 12:50:51,205 - INFO - |    |    ✔ CSV loaded ((233118, 18)), Size: 95.51 MB\n",
      "2025-05-16 12:50:51,215 - INFO - |    |    ✔ NPY loaded ((233118, 57, 3)), Size: 318.91 MB\n",
      "2025-05-16 12:50:51,330 - INFO - |    |    ✔ Sample loaded. Final shape: (233118, 57, 3)\n",
      "2025-05-16 12:50:51,330 - INFO - |    ├─── 🔹 Dataset: test\n",
      "2025-05-16 12:50:51,526 - INFO - |    |    ✔ CSV loaded ((233118, 18)), Size: 95.52 MB\n",
      "2025-05-16 12:50:51,529 - INFO - |    |    ✔ NPY loaded ((233118, 57, 3)), Size: 318.91 MB\n",
      "2025-05-16 12:50:51,641 - INFO - |    |    ✔ Sample loaded. Final shape: (233118, 57, 3)\n",
      "2025-05-16 12:50:51,642 - INFO - ├─── ✅ Finished loading all DESI datasets.\n",
      "2025-05-16 12:50:51,647 - INFO - ├ Concatenating DESI datasets...\n",
      "2025-05-16 12:50:51,647 - INFO - ├─── 🔄 Concatenating DESI dataset splits...\n",
      "2025-05-16 12:50:51,648 - INFO - |    |    Identified split names: ['train', 'val', 'test']\n",
      "2025-05-16 12:50:51,807 - INFO - |    |    Merged NPY arrays into 'all_np' with shape (792600, 57, 3)\n",
      "2025-05-16 12:50:51,807 - INFO - ├─── ✅ DESI split concatenation complete.\n",
      "2025-05-16 12:50:51,807 - INFO - ✅ Finished `load_dsets()`\n"
     ]
    }
   ],
   "source": [
    "DATA = loading_tools.load_dsets(root_path=root_path, datasets_jpas=load_JPAS_data, datasets_desi=load_DESI_data, random_seed=random_seed_load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_clean_data_options = {\n",
    "    \"apply_masks\"         : [\"unreliable\", \"magic_numbers\", \"negative_errors\", \"nan_values\", \"apply_additional_filters\"],\n",
    "    \"mask_indices\"        : [0, -2],\n",
    "    \"magic_numbers\"       : [-99, 99],\n",
    "    \"i_band_sn_threshold\" : 0,\n",
    "    \"z_lim_QSO_cut\"       : 2.2\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-16 12:50:51,816 - INFO - 🧽 Cleaning and masking data...\n",
      "2025-05-16 12:50:51,816 - INFO - ├── remove_invalid_NaN_rows()\n",
      "2025-05-16 12:50:52,002 - INFO - │   ├── # objects filled with NaNs in JPAS: 0(0.0%)\n",
      "2025-05-16 12:50:52,003 - INFO - │   ├── # objects filled with NaNs in DESI: 505(0.06%)\n",
      "2025-05-16 12:50:52,466 - INFO - ├── 🧹 Deleted cleaned DATA_clean dictionary to free memory.\n",
      "2025-05-16 12:50:52,466 - INFO - ├── apply_additional_filters()\n",
      "2025-05-16 12:50:52,475 - INFO - │   ├── JPAS: 52020 valid rows (S/N ≥ 0) (100.0%)\n",
      "2025-05-16 12:50:52,476 - INFO - │   ├── DESI: 792095 valid rows (S/N ≥ 0) (100.0%)\n",
      "2025-05-16 12:50:52,739 - INFO - │   ├── Additional filters applied successfully.\n",
      "2025-05-16 12:50:52,741 - INFO - ├── Masking out indices [0, -2] (unreliable in DESI).\n",
      "2025-05-16 12:50:53,219 - INFO - │   ├── Updated JPAS obs/err shape: (52020, 55)\n",
      "2025-05-16 12:50:53,219 - INFO - │   ├── Updated DESI mean/err shape: (792095, 55)\n",
      "2025-05-16 12:50:53,220 - INFO - ├── Checking for magic numbers (99 and -99) in datasets.\n",
      "2025-05-16 12:50:53,273 - INFO - │   ├── # objects containing some -99 entry in JPAS: 0(0.0%)\n",
      "2025-05-16 12:50:53,275 - INFO - │   ├── # objects containing some 99 entry in JPAS: 0(0.0%)\n",
      "2025-05-16 12:50:53,308 - INFO - │   ├── # objects containing some -99 entry in DESI: 99(0.01%)\n",
      "2025-05-16 12:50:53,340 - INFO - │   ├── # objects containing some 99 entry in DESI: 0(0.0%)\n",
      "2025-05-16 12:50:53,360 - INFO - ├── Checking for negative errors in datasets.\n",
      "2025-05-16 12:50:53,386 - INFO - │   ├── # objects containing some negative error entry in JPAS: 21521(41.37%)\n",
      "2025-05-16 12:50:53,418 - INFO - │   ├── # objects containing some negative error entry in DESI: 0(0.0%)\n",
      "2025-05-16 12:50:53,469 - INFO - ├── Splitting between High and Low z QSOs\n",
      "2025-05-16 12:50:54,247 - INFO - ├── 🔑 Starting encoding process for string list...\n",
      "2025-05-16 12:50:54,445 - INFO - |    ├── 📌 New Mapping Created: {np.str_('GALAXY'): 0, np.str_('QSO_high'): 1, np.str_('QSO_low'): 2, np.str_('STAR'): 3}\n",
      "2025-05-16 12:50:54,446 - INFO - ├── Encoding complete (4 categories).\n",
      "2025-05-16 12:50:54,446 - INFO - ├── 🔑 Starting encoding process for string list...\n",
      "2025-05-16 12:50:54,600 - INFO - |    ├── 📌 Used Provided Mapping: {np.str_('GALAXY'): 0, np.str_('QSO_high'): 1, np.str_('QSO_low'): 2, np.str_('STAR'): 3}\n",
      "2025-05-16 12:50:54,601 - INFO - ├── Encoding complete (4 categories).\n",
      "2025-05-16 12:50:54,601 - INFO - ├── 🔑 Starting encoding process for string list...\n",
      "2025-05-16 12:50:54,612 - INFO - |    ├── 📌 Used Provided Mapping: {np.str_('GALAXY'): 0, np.str_('QSO_high'): 1, np.str_('QSO_low'): 2, np.str_('STAR'): 3}\n",
      "2025-05-16 12:50:54,612 - INFO - ├── Encoding complete (4 categories).\n",
      "2025-05-16 12:50:54,615 - INFO - ├── 🔑 Starting encoding process for string list...\n",
      "2025-05-16 12:50:54,798 - INFO - |    ├── 📌 New Mapping Created: {np.str_('DEV'): 0, np.str_('EXP'): 1, np.str_('GGAL'): 2, np.str_('GPSF'): 3, np.str_('PSF'): 4, np.str_('REX'): 5, np.str_('SER'): 6, np.str_('nan'): 7}\n",
      "2025-05-16 12:50:54,798 - INFO - ├── Encoding complete (8 categories).\n",
      "2025-05-16 12:50:54,798 - INFO - ├── 🔑 Starting encoding process for string list...\n",
      "2025-05-16 12:50:54,944 - INFO - |    ├── 📌 Used Provided Mapping: {np.str_('DEV'): 0, np.str_('EXP'): 1, np.str_('GGAL'): 2, np.str_('GPSF'): 3, np.str_('PSF'): 4, np.str_('REX'): 5, np.str_('SER'): 6, np.str_('nan'): 7}\n",
      "2025-05-16 12:50:54,944 - INFO - ├── Encoding complete (8 categories).\n",
      "2025-05-16 12:50:54,944 - INFO - ├── 🔑 Starting encoding process for string list...\n",
      "2025-05-16 12:50:54,954 - INFO - |    ├── 📌 Used Provided Mapping: {np.str_('DEV'): 0, np.str_('EXP'): 1, np.str_('GGAL'): 2, np.str_('GPSF'): 3, np.str_('PSF'): 4, np.str_('REX'): 5, np.str_('SER'): 6, np.str_('nan'): 7}\n",
      "2025-05-16 12:50:54,955 - INFO - ├── Encoding complete (8 categories).\n",
      "2025-05-16 12:50:54,955 - INFO - ✅ Finished clean_and_mask_data()\n"
     ]
    }
   ],
   "source": [
    "DATA = cleaning_tools.clean_and_mask_data(\n",
    "    DATA=DATA,\n",
    "    apply_masks=dict_clean_data_options[\"apply_masks\"],\n",
    "    mask_indices=dict_clean_data_options[\"mask_indices\"],\n",
    "    magic_numbers=dict_clean_data_options[\"magic_numbers\"],\n",
    "    i_band_sn_threshold=dict_clean_data_options[\"i_band_sn_threshold\"],\n",
    "    z_lim_QSO_cut=dict_clean_data_options[\"z_lim_QSO_cut\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-16 12:50:54,966 - INFO - 🔍 crossmatch_IDs_two_datasets()...\n",
      "2025-05-16 12:50:54,967 - INFO - ├── 🚀 Starting ID categorization process...\n",
      "2025-05-16 12:50:54,976 - INFO - |    ├── 📌 Found 804570 unique IDs across 2 arrays.\n",
      "2025-05-16 12:50:55,125 - INFO - |    ├── Presence matrix created with shape: (2, 804570)\n",
      "2025-05-16 12:50:55,127 - INFO - |    ├── Category mask created with shape: (2, 804570)\n",
      "2025-05-16 12:50:55,127 - INFO - ├── 🚀 Starting index retrieval process...\n",
      "2025-05-16 12:50:55,127 - INFO - |    ├── 📌 Processing 804570 unique IDs across 2 arrays.\n",
      "2025-05-16 12:50:55,406 - INFO - ├── 🚀 Starting post-processing of unique IDs across two arrays...\n",
      "2025-05-16 12:50:55,419 - INFO - |    ├── Processing complete: 752550 IDs only in Array 1 (93.53%).\n",
      "2025-05-16 12:50:55,420 - INFO - |    ├── Processing complete: 24788 IDs only in Array 2 (3.08%).\n",
      "2025-05-16 12:50:55,420 - INFO - |    ├── Processing complete: 27232 IDs in both arrays (3.38%).\n",
      "2025-05-16 12:50:55,420 - INFO - ✅ Finished crossmatch_IDs_two_datasets()\n"
     ]
    }
   ],
   "source": [
    "Dict_LoA = {\"both\":{}, \"only\":{}} # Dictionary of Lists of Arrays (LoA) indicating, for each TARGETID, the associatted entries in the arrays, e.g. TARGETID[LoA[ii][0]] == TARGETID[LoA[ii][-1]]\n",
    "(\n",
    "    IDs_only_DESI, IDs_only_JPAS, IDs_both,\n",
    "    Dict_LoA[\"only\"][\"DESI\"], Dict_LoA[\"only\"][\"JPAS\"],\n",
    "    Dict_LoA[\"both\"][\"DESI\"], Dict_LoA[\"both\"][\"JPAS\"]\n",
    ") = crossmatch_tools.crossmatch_IDs_two_datasets(\n",
    "    DATA[\"DESI\"]['TARGETID'], DATA[\"JPAS\"]['TARGETID']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_split_data_options = {\n",
    "    \"train_ratio_both\"             : 0.8,\n",
    "    \"val_ratio_both\"               : 0.1,\n",
    "    \"test_ratio_both\"              : 0.1,\n",
    "    \"random_seed_split_both\"       : 42,\n",
    "    \"train_ratio_only_DESI\"        : 0.8,\n",
    "    \"val_ratio_only_DESI\"          : 0.1,\n",
    "    \"test_ratio_only_DESI\"         : 0.1,\n",
    "    \"random_seed_split_only_DESI\"  : 42\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-16 12:50:55,452 - INFO - ├── ✂️ Splitting list of arrays (LoA) into train/val/test subsets...\n",
      "2025-05-16 12:50:55,456 - INFO - ├── Finished splitting.\n",
      "2025-05-16 12:50:55,457 - INFO - ├── ✂️ Splitting list of arrays (LoA) into train/val/test subsets...\n",
      "2025-05-16 12:50:55,460 - INFO - ├── Finished splitting.\n",
      "2025-05-16 12:50:55,461 - INFO - ├── ✂️ Splitting list of arrays (LoA) into train/val/test subsets...\n",
      "2025-05-16 12:50:55,577 - INFO - ├── Finished splitting.\n"
     ]
    }
   ],
   "source": [
    "# Split the Lists of Arrays into training, validation, and testing sets\n",
    "Dict_LoA_split = {\"both\":{}, \"only\":{}}\n",
    "Dict_LoA_split[\"both\"][\"JPAS\"] = process_dset_splits.split_LoA(\n",
    "    Dict_LoA[\"both\"][\"JPAS\"],\n",
    "    train_ratio = dict_split_data_options[\"train_ratio_both\"],\n",
    "    val_ratio = dict_split_data_options[\"val_ratio_both\"],\n",
    "    test_ratio = dict_split_data_options[\"test_ratio_both\"],\n",
    "    seed = dict_split_data_options[\"random_seed_split_both\"]\n",
    ")\n",
    "Dict_LoA_split[\"both\"][\"DESI\"] = process_dset_splits.split_LoA(\n",
    "    Dict_LoA[\"both\"][\"DESI\"],\n",
    "    train_ratio = dict_split_data_options[\"train_ratio_both\"],\n",
    "    val_ratio = dict_split_data_options[\"val_ratio_both\"],\n",
    "    test_ratio = dict_split_data_options[\"test_ratio_both\"],\n",
    "    seed = dict_split_data_options[\"random_seed_split_both\"]\n",
    ")\n",
    "Dict_LoA_split[\"only\"][\"DESI\"]  = process_dset_splits.split_LoA(\n",
    "    Dict_LoA[\"only\"][\"DESI\"],\n",
    "    train_ratio = dict_split_data_options[\"train_ratio_only_DESI\"],\n",
    "    val_ratio = dict_split_data_options[\"val_ratio_only_DESI\"],\n",
    "    test_ratio = dict_split_data_options[\"test_ratio_only_DESI\"],\n",
    "    seed = dict_split_data_options[\"random_seed_split_only_DESI\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys_xx = ['OBS', 'ERR', 'MORPHTYPE_int']\n",
    "keys_yy = ['SPECTYPE_int', 'TARGETID']\n",
    "normalize = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-16 12:50:55,586 - INFO - ⚙️ Preparing split: train\n",
      "2025-05-16 12:50:55,587 - INFO - ├── DESI_combined\n",
      "2025-05-16 12:50:55,587 - INFO - |    ├── 🔧 extract_and_combine_DESI_data()\n",
      "2025-05-16 12:50:55,587 - INFO - |    ├── Extracting features and labels from DESI-only subset...\n",
      "2025-05-16 12:50:57,326 - INFO - |    ├── Extracting features and labels from DESI-matched subset...\n",
      "2025-05-16 12:50:57,492 - INFO - |    ├── Applied index shift of 611345 to matched DESI group to ensure uniqueness\n",
      "2025-05-16 12:50:57,562 - INFO - |    ├── Finished extract_and_combine_DESI_data()\n",
      "2025-05-16 12:50:57,567 - INFO - ├── 💿 Initializing DataLoader object with 633698 samples...\n",
      "2025-05-16 12:50:57,970 - INFO - ├── ✔ Finished Initialization. Class distribution: [0: 446536 (70.47%), 1: 9517 (1.50%), 2: 27045 (4.27%), 3: 150600 (23.77%)]\n",
      "2025-05-16 12:50:57,970 - INFO - ├── DESI_matched\n",
      "2025-05-16 12:50:57,970 - INFO - |    ├── 🔧 extract_data_matched()\n",
      "2025-05-16 12:50:57,970 - INFO - |    ├── Extracting features and labels from matched dataset...\n",
      "2025-05-16 12:50:58,026 - INFO - |    ├── Finished extract_data_matched()\n",
      "2025-05-16 12:50:58,046 - INFO - ├── 💿 Initializing DataLoader object with 22353 samples...\n",
      "2025-05-16 12:50:58,051 - INFO - ├── ✔ Finished Initialization. Class distribution: [0: 15704 (70.25%), 1: 287 (1.28%), 2: 930 (4.16%), 3: 5432 (24.30%)]\n",
      "2025-05-16 12:50:58,051 - INFO - ├── JPAS_matched\n",
      "2025-05-16 12:50:58,051 - INFO - |    ├── 🔧 extract_data_matched()\n",
      "2025-05-16 12:50:58,052 - INFO - |    ├── Extracting features and labels from matched dataset...\n",
      "2025-05-16 12:50:58,093 - INFO - |    ├── Finished extract_data_matched()\n",
      "2025-05-16 12:50:58,095 - INFO - ├── 💿 Initializing DataLoader object with 21785 samples...\n",
      "2025-05-16 12:50:58,099 - INFO - ├── ✔ Finished Initialization. Class distribution: [0: 15409 (70.73%), 1: 269 (1.23%), 2: 869 (3.99%), 3: 5238 (24.04%)]\n",
      "2025-05-16 12:50:58,100 - INFO - ⚙️ Preparing split: val\n",
      "2025-05-16 12:50:58,100 - INFO - ├── DESI_combined\n",
      "2025-05-16 12:50:58,100 - INFO - |    ├── 🔧 extract_and_combine_DESI_data()\n",
      "2025-05-16 12:50:58,100 - INFO - |    ├── Extracting features and labels from DESI-only subset...\n",
      "2025-05-16 12:50:58,333 - INFO - |    ├── Extracting features and labels from DESI-matched subset...\n",
      "2025-05-16 12:50:58,355 - INFO - |    ├── Applied index shift of 76398 to matched DESI group to ensure uniqueness\n",
      "2025-05-16 12:50:58,366 - INFO - |    ├── Finished extract_and_combine_DESI_data()\n",
      "2025-05-16 12:50:58,368 - INFO - ├── 💿 Initializing DataLoader object with 79190 samples...\n",
      "2025-05-16 12:50:58,391 - INFO - ├── ✔ Finished Initialization. Class distribution: [0: 55860 (70.54%), 1: 1191 (1.50%), 2: 3415 (4.31%), 3: 18724 (23.64%)]\n",
      "2025-05-16 12:50:58,391 - INFO - ├── DESI_matched\n",
      "2025-05-16 12:50:58,392 - INFO - |    ├── 🔧 extract_data_matched()\n",
      "2025-05-16 12:50:58,392 - INFO - |    ├── Extracting features and labels from matched dataset...\n",
      "2025-05-16 12:50:58,399 - INFO - |    ├── Finished extract_data_matched()\n",
      "2025-05-16 12:50:58,402 - INFO - ├── 💿 Initializing DataLoader object with 2792 samples...\n",
      "2025-05-16 12:50:58,403 - INFO - ├── ✔ Finished Initialization. Class distribution: [0: 1982 (70.99%), 1: 38 (1.36%), 2: 117 (4.19%), 3: 655 (23.46%)]\n",
      "2025-05-16 12:50:58,403 - INFO - ├── JPAS_matched\n",
      "2025-05-16 12:50:58,403 - INFO - |    ├── 🔧 extract_data_matched()\n",
      "2025-05-16 12:50:58,403 - INFO - |    ├── Extracting features and labels from matched dataset...\n",
      "2025-05-16 12:50:58,410 - INFO - |    ├── Finished extract_data_matched()\n",
      "2025-05-16 12:50:58,410 - INFO - ├── 💿 Initializing DataLoader object with 2723 samples...\n",
      "2025-05-16 12:50:58,411 - INFO - ├── ✔ Finished Initialization. Class distribution: [0: 1948 (71.54%), 1: 35 (1.29%), 2: 111 (4.08%), 3: 629 (23.10%)]\n",
      "2025-05-16 12:50:58,411 - INFO - ⚙️ Preparing split: test\n",
      "2025-05-16 12:50:58,411 - INFO - ├── DESI_combined\n",
      "2025-05-16 12:50:58,411 - INFO - |    ├── 🔧 extract_and_combine_DESI_data()\n",
      "2025-05-16 12:50:58,411 - INFO - |    ├── Extracting features and labels from DESI-only subset...\n",
      "2025-05-16 12:50:58,642 - INFO - |    ├── Extracting features and labels from DESI-matched subset...\n",
      "2025-05-16 12:50:58,665 - INFO - |    ├── Applied index shift of 76414 to matched DESI group to ensure uniqueness\n",
      "2025-05-16 12:50:58,676 - INFO - |    ├── Finished extract_and_combine_DESI_data()\n",
      "2025-05-16 12:50:58,677 - INFO - ├── 💿 Initializing DataLoader object with 79207 samples...\n",
      "2025-05-16 12:50:58,700 - INFO - ├── ✔ Finished Initialization. Class distribution: [0: 55684 (70.30%), 1: 1205 (1.52%), 2: 3479 (4.39%), 3: 18839 (23.78%)]\n",
      "2025-05-16 12:50:58,700 - INFO - ├── DESI_matched\n",
      "2025-05-16 12:50:58,700 - INFO - |    ├── 🔧 extract_data_matched()\n",
      "2025-05-16 12:50:58,700 - INFO - |    ├── Extracting features and labels from matched dataset...\n",
      "2025-05-16 12:50:58,708 - INFO - |    ├── Finished extract_data_matched()\n",
      "2025-05-16 12:50:58,711 - INFO - ├── 💿 Initializing DataLoader object with 2793 samples...\n",
      "2025-05-16 12:50:58,711 - INFO - ├── ✔ Finished Initialization. Class distribution: [0: 1956 (70.03%), 1: 33 (1.18%), 2: 140 (5.01%), 3: 664 (23.77%)]\n",
      "2025-05-16 12:50:58,711 - INFO - ├── JPAS_matched\n",
      "2025-05-16 12:50:58,712 - INFO - |    ├── 🔧 extract_data_matched()\n",
      "2025-05-16 12:50:58,712 - INFO - |    ├── Extracting features and labels from matched dataset...\n",
      "2025-05-16 12:50:58,718 - INFO - |    ├── Finished extract_data_matched()\n",
      "2025-05-16 12:50:58,718 - INFO - ├── 💿 Initializing DataLoader object with 2724 samples...\n",
      "2025-05-16 12:50:58,719 - INFO - ├── ✔ Finished Initialization. Class distribution: [0: 1914 (70.26%), 1: 32 (1.17%), 2: 133 (4.88%), 3: 645 (23.68%)]\n"
     ]
    }
   ],
   "source": [
    "dset_loaders = {\"DESI_combined\":{}, \"DESI_matched\":{}, \"JPAS_matched\":{}}\n",
    "provided_normalization = None\n",
    "for key_dset in [\"train\", \"val\", \"test\"]:\n",
    "    logging.info(f\"⚙️ Preparing split: {key_dset}\")\n",
    "\n",
    "    # DESI combined (only + matched)\n",
    "    logging.info(\"├── DESI_combined\")\n",
    "    LoA, xx, yy = process_dset_splits.extract_and_combine_DESI_data(\n",
    "        Dict_LoA_split[\"only\"][\"DESI\"][key_dset],\n",
    "        Dict_LoA_split[\"both\"][\"DESI\"][key_dset],\n",
    "        DATA[\"DESI\"], keys_xx, keys_yy\n",
    "    )\n",
    "    dset_loaders[\"DESI_combined\"][key_dset] = data_loaders.DataLoader(\n",
    "        xx, yy, normalize=normalize, provided_normalization=provided_normalization\n",
    "    )\n",
    "    if key_dset == \"train\":\n",
    "        provided_normalization = (\n",
    "            dset_loaders[\"DESI_combined\"][\"train\"].means,\n",
    "            dset_loaders[\"DESI_combined\"][\"train\"].stds\n",
    "        )\n",
    "\n",
    "    # DESI matched\n",
    "    logging.info(\"├── DESI_matched\")\n",
    "    LoA, xx, yy = process_dset_splits.extract_data_matched(\n",
    "        Dict_LoA_split[\"both\"][\"DESI\"][key_dset],\n",
    "        DATA[\"DESI\"], keys_xx, keys_yy\n",
    "    )\n",
    "    dset_loaders[\"DESI_matched\"][key_dset] = data_loaders.DataLoader(\n",
    "        xx, yy, normalize=normalize, provided_normalization=provided_normalization\n",
    "    )\n",
    "\n",
    "    # JPAS matched\n",
    "    logging.info(\"├── JPAS_matched\")\n",
    "    LoA, xx, yy = process_dset_splits.extract_data_matched(\n",
    "        Dict_LoA_split[\"both\"][\"JPAS\"][key_dset],\n",
    "        DATA[\"JPAS\"], keys_xx, keys_yy\n",
    "    )\n",
    "    dset_loaders[\"JPAS_matched\"][key_dset] = data_loaders.DataLoader(\n",
    "        xx, yy, normalize=normalize, provided_normalization=provided_normalization\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "(22353,)\n",
      "(21785,)\n",
      "(21785,)\n",
      "(21785,)\n",
      "True\n",
      "\n",
      "val\n",
      "(2792,)\n",
      "(2723,)\n",
      "(2723,)\n",
      "(2723,)\n",
      "True\n",
      "\n",
      "test\n",
      "(2793,)\n",
      "(2724,)\n",
      "(2724,)\n",
      "(2724,)\n",
      "True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for key_dset in [\"train\", \"val\", \"test\"]:\n",
    "    print(key_dset)\n",
    "    tmp_DESI_TARGETID = dset_loaders[\"DESI_matched\"][key_dset].yy[\"TARGETID\"]\n",
    "    tmp_JPAS_TARGETID = dset_loaders[\"JPAS_matched\"][key_dset].yy[\"TARGETID\"]\n",
    "\n",
    "    print(tmp_DESI_TARGETID.shape)\n",
    "    print(tmp_JPAS_TARGETID.shape)\n",
    "\n",
    "    print(np.unique(tmp_DESI_TARGETID).shape)\n",
    "    print(np.unique(tmp_JPAS_TARGETID).shape)\n",
    "\n",
    "    print(np.array_equal(\n",
    "        np.sort(np.unique(tmp_DESI_TARGETID)),\n",
    "        np.sort(np.unique(tmp_JPAS_TARGETID))\n",
    "    ))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
