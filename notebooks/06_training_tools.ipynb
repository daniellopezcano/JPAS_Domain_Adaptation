{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s') # NOTSET, DEBUG, INFO, WARN, ERROR, CRITICAL\n",
    "\n",
    "from JPAS_DA import global_setup\n",
    "from JPAS_DA.data import data_loaders\n",
    "from JPAS_DA.data import generate_toy_data\n",
    "from JPAS_DA.models import model_building_tools\n",
    "from JPAS_DA.training import training_tools\n",
    "from JPAS_DA.training import save_load_tools\n",
    "from JPAS_DA.evaluation import evaluation_tools\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "from JPAS_DA.utils import plotting_utils\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('default')\n",
    "plt.close('all')\n",
    "font, rcnew = plotting_utils.matplotlib_default_config()\n",
    "mpl.rc('font', **font)\n",
    "plt.rcParams.update(rcnew)\n",
    "plt.style.use('tableau-colorblind10')\n",
    "%matplotlib widget\n",
    "\n",
    "from JPAS_DA.utils import aux_tools\n",
    "aux_tools.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Shared Parameters\n",
    "# =========================\n",
    "n_classes = 4\n",
    "class_proportions = np.array([0.2, 0.2, 0.55, 0.05])\n",
    "assert np.isclose(class_proportions.sum(), 1.0)\n",
    "\n",
    "# sample sizes\n",
    "n_samples_train    = 32768\n",
    "n_samples_val      = 32768\n",
    "n_samples_test     = 32768\n",
    "n_samples_train_DA = 1024\n",
    "n_samples_val_DA   = 1024\n",
    "\n",
    "# seeds\n",
    "seed_structure = 137\n",
    "seed_train     = 42\n",
    "seed_val       = 276\n",
    "seed_test      = 0\n",
    "seed_train_DA  = 1\n",
    "seed_val_DA    = 2\n",
    "seed_transform = 3\n",
    "\n",
    "# =========================\n",
    "# Create specs\n",
    "# =========================\n",
    "specs_target = [\n",
    "    generate_toy_data.spec_mixture([\n",
    "        generate_toy_data.spec_gaussian(center=[0.0, 0.0], sigma=(1.5, 0.2), angle=np.pi/4),\n",
    "    ], weights=[1.0]),\n",
    "    generate_toy_data.spec_mixture([\n",
    "        generate_toy_data.spec_gaussian(center=[-1.0, -1.0], sigma=(0.15, 0.9)),\n",
    "    ], weights=[1.0]),\n",
    "    generate_toy_data.spec_mixture([\n",
    "        generate_toy_data.spec_gaussian(center=[2.5, 2.5], sigma=(0.6, 0.6)),\n",
    "        generate_toy_data.spec_gaussian(center=[-3.6, -2.0], sigma=(0.8, 0.8)),\n",
    "    ], weights=[0.7, 0.3]),\n",
    "    generate_toy_data.spec_mixture([\n",
    "        generate_toy_data.spec_ring(center=[-3.5, -2.0], radius=1.1, width=0.05),\n",
    "    ], weights=[1.0]),\n",
    "]\n",
    "\n",
    "specs_source = [\n",
    "    generate_toy_data.spec_mixture([\n",
    "        generate_toy_data.spec_gaussian(center=[0.0, -1.0], sigma=(0.9, 0.2)),\n",
    "    ], weights=[1.0]),\n",
    "    generate_toy_data.spec_mixture([\n",
    "        generate_toy_data.spec_gaussian(center=[-1.0, -1.0], sigma=(0.15, 0.9)),\n",
    "    ], weights=[1.0]),\n",
    "    generate_toy_data.spec_mixture([\n",
    "        generate_toy_data.spec_gaussian(center=[2.5, 2.5], sigma=(0.6, 0.6)),\n",
    "        generate_toy_data.spec_gaussian(center=[-3.5, -2.0], sigma=(0.4, 0.4)),\n",
    "    ], weights=[0.5, 0.5]),\n",
    "    generate_toy_data.spec_mixture([\n",
    "        generate_toy_data.spec_ring(center=[-3.5, -2.0], radius=1.1, width=0.1),\n",
    "    ], weights=[1.0]),\n",
    "]\n",
    "\n",
    "# =========================\n",
    "# Generate Train/Val Source with the SAME shared specs and Target/Test with DIFFERENT shifted specs\n",
    "# =========================\n",
    "xx_train, yy_train, train_counts = generate_toy_data.generate_dataset_from_specs(\n",
    "    n_samples_train, specs_source, class_proportions, seed=seed_train\n",
    ")\n",
    "xx_val, yy_val, val_counts = generate_toy_data.generate_dataset_from_specs(\n",
    "    n_samples_val, specs_source, class_proportions, seed=seed_val\n",
    ")\n",
    "xx_test, yy_test, test_counts = generate_toy_data.generate_dataset_from_specs(\n",
    "    n_samples_test, specs_target, class_proportions, seed=seed_test\n",
    ")\n",
    "xx_train_DA, yy_train_DA, _ = generate_toy_data.generate_dataset_from_specs(\n",
    "    n_samples_train_DA, specs_target, class_proportions, seed=seed_train_DA\n",
    ")\n",
    "xx_val_DA, yy_val_DA, _ = generate_toy_data.generate_dataset_from_specs(\n",
    "    n_samples_val_DA, specs_target, class_proportions, seed=seed_val_DA\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Visualize sets ===\n",
    "classes = list(range(n_classes))\n",
    "cmap = plt.cm.get_cmap(\"plasma\", len(classes))  # discretize into N distinct colors\n",
    "colors = cmap(np.linspace(0, 1, len(classes), endpoint=False))\n",
    "color_dict = {cls: col for cls, col in zip(classes, colors)}\n",
    "\n",
    "x_min, x_max = -6, 6\n",
    "y_min, y_max = -6, 6\n",
    "\n",
    "fig1, ax_main1 = plotting_utils.plot_2d_classification_with_kde(\n",
    "    xx_train[\"OBS\"], yy_train[\"SPECTYPE_int\"], title=\"Source (Train Set)\", class_color_dict=color_dict\n",
    ")\n",
    "ax_main1.set_xlim([x_min, x_max])\n",
    "ax_main1.set_ylim([y_min, y_max])\n",
    "plt.show()\n",
    "\n",
    "# fig2, ax_main2 = plotting_utils.plot_2d_classification_with_kde(\n",
    "# xx_val[\"OBS\"], yy_val[\"SPECTYPE_int\"], title=\"Source (Validation Set)\", class_color_dict=color_dict\n",
    "# )\n",
    "ax_main1.set_xlim([x_min, x_max])\n",
    "ax_main1.set_ylim([y_min, y_max])\n",
    "# plt.show()\n",
    "\n",
    "fig3, ax_main3 = plotting_utils.plot_2d_classification_with_kde(\n",
    "    xx_test[\"OBS\"], yy_test[\"SPECTYPE_int\"], title=\"Target (Test Set)\", class_color_dict=color_dict\n",
    ")\n",
    "ax_main3.set_xlim([x_min, x_max])\n",
    "ax_main3.set_ylim([y_min, y_max])\n",
    "plt.show()\n",
    "\n",
    "fig4, ax_main4 = plotting_utils.plot_2d_classification_with_kde(\n",
    "    xx_train_DA[\"OBS\"], yy_train_DA[\"SPECTYPE_int\"], title=\"Target (Train Set)\", class_color_dict=color_dict\n",
    ")\n",
    "ax_main4.set_xlim([x_min, x_max])\n",
    "ax_main4.set_ylim([y_min, y_max])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_load = None # This will be employed when doing domain adaptation\n",
    "\n",
    "path_load_encoder = os.path.join(path_load, \"model_encoder.pt\") if path_load else None\n",
    "path_load_downstream = os.path.join(path_load, \"model_downstream.pt\") if path_load else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset_train = data_loaders.DataLoader(xx_train, yy_train)\n",
    "dset_val = data_loaders.DataLoader(xx_val, yy_val)\n",
    "hidden_layers_encoder = [24, 16, 8]\n",
    "dropout_rates_encoder = [0.01, 0.01, 0.01]\n",
    "output_dim_encoder = 2\n",
    "use_batchnorm_encoder = False\n",
    "hidden_layers_downstream = [16, 16]\n",
    "dropout_rates_downstream = [0.01, 0.01]\n",
    "use_batchnorm_downstream = False\n",
    "batch_size_training = int(dset_train.NN_xx/128)\n",
    "batch_size_val = np.min([n_samples_train_DA, n_samples_val_DA])\n",
    "\n",
    "\n",
    "# dset_train = data_loaders.DataLoader(xx_train_DA, yy_train_DA)\n",
    "# dset_val = data_loaders.DataLoader(xx_val_DA, yy_val_DA)\n",
    "# hidden_layers_encoder = [16, 16]\n",
    "# dropout_rates_encoder = [0.01, 0.01]\n",
    "# output_dim_encoder = 2\n",
    "# use_batchnorm_encoder = True\n",
    "# hidden_layers_downstream = [16]\n",
    "# dropout_rates_downstream = [0.01]\n",
    "# use_batchnorm_downstream = True\n",
    "# batch_size_training = int(dset_train.NN_xx/1)\n",
    "# batch_size_val = np.min([n_samples_train_DA, n_samples_val_DA])\n",
    "\n",
    "\n",
    "dset_test = data_loaders.DataLoader(xx_test, yy_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Load or build encoder ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "if path_load_encoder:\n",
    "    assert os.path.isfile(path_load_encoder), f\"‚ùå Encoder checkpoint not found: {path_load_encoder}\"\n",
    "    logging.info(f\"üì• Loading encoder from checkpoint: {path_load_encoder}\")\n",
    "\n",
    "    config_encoder, model_encoder = save_load_tools.load_model_from_checkpoint(\n",
    "        path_load_encoder,\n",
    "        model_building_tools.create_mlp,\n",
    "        override_dropout=dropout_rates_encoder,\n",
    "        use_batchnorm=use_batchnorm_encoder,\n",
    "    )\n",
    "else:\n",
    "    logging.info(\"üõ†Ô∏è Building encoder model from configuration...\")\n",
    "    config_encoder = {\n",
    "        'input_dim': dset_train.xx['OBS'].shape[-1],\n",
    "        'hidden_layers': hidden_layers_encoder,\n",
    "        'dropout_rates': dropout_rates_encoder,\n",
    "        'output_dim': output_dim_encoder,\n",
    "        'use_batchnorm': use_batchnorm_encoder,\n",
    "        'use_layernorm_at_output': False,\n",
    "        'init_method': 'xavier'\n",
    "    }\n",
    "    model_encoder = model_building_tools.create_mlp(**config_encoder)\n",
    "\n",
    "model_encoder.eval()\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Load or build downstream model ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "if path_load_downstream:\n",
    "    assert os.path.isfile(path_load_downstream), f\"‚ùå Downstream checkpoint not found: {path_load_downstream}\"\n",
    "    logging.info(f\"üì• Loading downstream model from checkpoint: {path_load_downstream}\")\n",
    "\n",
    "    config_downstream, model_downstream = save_load_tools.load_model_from_checkpoint(\n",
    "        path_load_downstream,\n",
    "        model_building_tools.create_mlp,\n",
    "        override_dropout=dropout_rates_downstream,\n",
    "        use_batchnorm=use_batchnorm_downstream,\n",
    "    )\n",
    "else:\n",
    "    logging.info(\"üõ†Ô∏è Building downstream model from configuration...\")\n",
    "    config_downstream = {\n",
    "        'input_dim': output_dim_encoder,\n",
    "        'hidden_layers': hidden_layers_downstream,\n",
    "        'dropout_rates': dropout_rates_downstream,\n",
    "        'output_dim': n_classes,\n",
    "        'use_batchnorm': use_batchnorm_downstream,\n",
    "        'use_layernorm_at_output': False,\n",
    "        'init_method': 'xavier'\n",
    "    }\n",
    "    model_downstream = model_building_tools.create_mlp(**config_downstream)\n",
    "\n",
    "model_downstream.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(model_encoder)\n",
    "# print(model_downstream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_strategy = \"true_random\"\n",
    "freeze_downstream_model = False\n",
    "\n",
    "path_save = global_setup.path_models\n",
    "path_save += \"/06_example_model\" # \"/06_example_model_Supervised\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if sampling_strategy == \"true_random\":\n",
    "    counts = dset_train.class_counts\n",
    "    total_samples = np.sum(counts)\n",
    "    weights = total_samples / (n_classes * counts)\n",
    "    class_weights = torch.tensor(weights, dtype=torch.float32)\n",
    "if sampling_strategy == \"class_random\":\n",
    "    class_weights = torch.tensor(np.ones(n_classes), dtype=torch.float32)\n",
    "\n",
    "loss_function_dict = {\n",
    "    \"type\": \"CrossEntropyLoss\",\n",
    "    \"sampling_strategy\": sampling_strategy,\n",
    "    \"class_weights\": class_weights,\n",
    "    \"l2_lambda\": 0.0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_val_loss = training_tools.train_model(\n",
    "    dset_train=dset_train,\n",
    "    model_encoder=model_encoder,\n",
    "    model_downstream=model_downstream,\n",
    "    loss_function_dict=loss_function_dict,\n",
    "    freeze_downstream_model=freeze_downstream_model,\n",
    "    dset_val=dset_val,\n",
    "    NN_epochs=1024,\n",
    "    NN_batches_per_epoch=1024,\n",
    "    batch_size= batch_size_training,\n",
    "    batch_size_val= batch_size_val,\n",
    "    lr=1e-2,\n",
    "    weight_decay=0.0001,\n",
    "    clip_grad_norm=1.0,\n",
    "    seed_mode=\"deterministic\",\n",
    "    seed=0,\n",
    "    path_save=path_save,\n",
    "    config_encoder=config_encoder,\n",
    "    config_downstream=config_downstream\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting_utils.plot_training_curves(path_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = next(model_encoder.parameters()).device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_labels = dset_train.class_labels\n",
    "\n",
    "# === Generate meshgrid from training data bounds ===\n",
    "\n",
    "grid_res = 128\n",
    "xx_vals = np.linspace(x_min, x_max, grid_res)\n",
    "yy_vals = np.linspace(y_min, y_max, grid_res)\n",
    "xx_mesh, yy_mesh = np.meshgrid(xx_vals, yy_vals)\n",
    "\n",
    "grid_points = np.stack([xx_mesh.ravel(), yy_mesh.ravel()], axis=1)\n",
    "xx_grid = torch.tensor(grid_points, dtype=torch.float32, device=device)\n",
    "\n",
    "# === Compute class probabilities and predicted class ===\n",
    "with torch.no_grad():\n",
    "    features_grid_points = model_encoder(xx_grid)\n",
    "    logits_grid_points = model_downstream(features_grid_points)\n",
    "    yy_pred_P_grid_points = torch.nn.functional.softmax(logits_grid_points, dim=1).cpu().numpy()\n",
    "    yy_pred_grid_points = np.argmax(yy_pred_P_grid_points, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Reshape to grid ===\n",
    "Z = yy_pred_grid_points.reshape(grid_res, grid_res).astype(float)  # cast to float for gouraud shading\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 7))\n",
    "\n",
    "# Discrete colormap (but visually smoothed by gouraud)\n",
    "unique_labels = np.unique(Z.astype(int))\n",
    "cmap_base = plt.cm.get_cmap(\"plasma\", len(unique_labels))   # discretize plasma into N colors\n",
    "colors = [cmap_base(i) for i in range(len(unique_labels))]  # i = 0..N-1\n",
    "class_color_dict = {label: col for label, col in zip(unique_labels, colors)}\n",
    "cmap = mpl.colors.ListedColormap([class_color_dict[label] for label in unique_labels])\n",
    "\n",
    "# Use boundaries to align ticks to color blocks\n",
    "boundaries = np.arange(len(unique_labels) + 1) - 0.5\n",
    "norm = mpl.colors.BoundaryNorm(boundaries, ncolors=len(unique_labels))\n",
    "pcm = ax.pcolormesh(xx_mesh, yy_mesh, Z, cmap=cmap, norm=norm, shading='nearest')\n",
    "\n",
    "# Overlay training points using the same colormap\n",
    "for ii, label in enumerate(unique_labels):\n",
    "\n",
    "    mask = dset_train.yy[\"SPECTYPE_int\"] == label\n",
    "    tmp = dset_train.xx[\"OBS\"][mask]\n",
    "    ax.scatter(tmp[:, 0], tmp[:, 1], marker='^', s=120, alpha=0.9, color=class_color_dict[label],\n",
    "               edgecolor='black', linewidth=0.4)\n",
    "    \n",
    "    mask = dset_val.yy[\"SPECTYPE_int\"] == label\n",
    "    tmp = dset_val.xx[\"OBS\"][mask]\n",
    "    ax.scatter(tmp[:, 0], tmp[:, 1], s=12, alpha=0.9, color=class_color_dict[label],\n",
    "               edgecolor='black', linewidth=0.4)\n",
    "\n",
    "# Styling\n",
    "ax.set_title(\"Predicted Class Map\", fontsize=18)\n",
    "ax.set_xlabel(r\"$\\mathrm{Feature~1}$\", fontsize=16)\n",
    "ax.set_ylabel(r\"$\\mathrm{Feature~2}$\", fontsize=16)\n",
    "ax.tick_params(labelsize=12)\n",
    "\n",
    "# Colorbar (optional: no strict boundaries here due to gouraud shading)\n",
    "cbar = fig.colorbar(pcm, ax=ax, orientation='vertical', fraction=0.046, pad=0.04)\n",
    "cbar.set_label(\"Predicted Class\", fontsize=14)\n",
    "cbar.ax.tick_params(labelsize=12)\n",
    "cbar.ax.set_yticks(np.arange(len(unique_labels)))\n",
    "cbar.ax.set_yticklabels(unique_labels)\n",
    "\n",
    "ax.set_xlim(x_min, x_max)\n",
    "ax.set_ylim(y_min, y_max)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Reshape to grid ===\n",
    "Z = yy_pred_P_grid_points.reshape(grid_res, grid_res, n_classes)\n",
    "\n",
    "# Determine a roughly square grid layout\n",
    "n_cols = np.ceil(np.sqrt(n_classes)).astype(int)\n",
    "n_rows = np.ceil(n_classes / n_cols).astype(int)\n",
    "\n",
    "fig, axs = plt.subplots(n_rows, n_cols, figsize=(4.5 * n_cols, 4.5 * n_rows), sharex=True, sharey=True)\n",
    "axs = np.array(axs).reshape(n_rows, n_cols)  # Ensure 2D array for indexing\n",
    "\n",
    "vmin, vmax = 0.0, 1.0\n",
    "cmap = \"seismic\"\n",
    "pcm = None\n",
    "\n",
    "for idx in range(n_classes):\n",
    "    row, col = divmod(idx, n_cols)\n",
    "    ax = axs[row, col]\n",
    "\n",
    "    pcm = ax.pcolormesh(xx_mesh, yy_mesh, Z[..., idx], cmap=cmap, shading='auto', vmin=vmin, vmax=vmax)\n",
    "\n",
    "    # Annotate class index\n",
    "    ax.text(\n",
    "        0.98, 0.98, f\"Class {idx}\", transform=ax.transAxes, ha='right', va='top', fontsize=14, fontweight='bold',\n",
    "        bbox=dict(facecolor='white', edgecolor='black', boxstyle='round,pad=0.3', alpha=0.85)\n",
    "    )\n",
    "\n",
    "    ax.set_xlabel(r\"$\\mathrm{Feature~1}$\", fontsize=16)\n",
    "    ax.set_ylabel(r\"$\\mathrm{Feature~2}$\", fontsize=16)\n",
    "    ax.tick_params(axis='both', labelsize=14)\n",
    "\n",
    "    for jj, label in enumerate(unique_labels):\n",
    "        mask = dset_train.yy[\"SPECTYPE_int\"] == label\n",
    "        tmp = dset_train.xx[\"OBS\"][mask]\n",
    "        color = 'limegreen' if idx == jj else 'k'\n",
    "        ax.scatter(tmp[:, 0], tmp[:, 1], s=12, alpha=0.4, color=color)\n",
    "\n",
    "# Hide unused axes\n",
    "for idx in range(n_classes, n_rows * n_cols):\n",
    "    row, col = divmod(idx, n_cols)\n",
    "    axs[row, col].axis(\"off\")\n",
    "\n",
    "# Adjust spacing between subplots\n",
    "plt.subplots_adjust(hspace=0.3, wspace=0.3)\n",
    "\n",
    "# === Shared horizontal colorbar at the top ===\n",
    "pos0 = axs[0, 0].get_position()\n",
    "posn = axs[-1, -1 if n_classes % n_cols != 0 else n_cols - 1].get_position()\n",
    "cbar_ax = fig.add_axes([pos0.x0, pos0.y1 + 0.04, posn.x1 - pos0.x0, 0.02])\n",
    "cbar = fig.colorbar(pcm, cax=cbar_ax, orientation='horizontal')\n",
    "cbar.ax.set_title(\"Predicted Class Probability\", fontsize=16, pad=8)\n",
    "cbar.ax.tick_params(labelsize=14)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Train ===\n",
    "xx_train, yy_true_train = dset_train(batch_size=dset_train.NN_xx, seed=0, sampling_strategy=\"true_random\", to_torch=True, device=device)\n",
    "\n",
    "with torch.no_grad():\n",
    "        features_train = model_encoder(xx_train)\n",
    "        logits = model_downstream(features_train)\n",
    "yy_pred_P_train = torch.nn.functional.softmax(logits, dim=1)\n",
    "yy_pred_P_train = yy_pred_P_train.cpu().numpy()\n",
    "yy_pred_train = np.argmax(yy_pred_P_train, axis=1)\n",
    "yy_true_train = yy_true_train.cpu().numpy()\n",
    "xx_train = xx_train.cpu().numpy()\n",
    "features_train = features_train.cpu().numpy()\n",
    "\n",
    "evaluation_tools.plot_confusion_matrix(\n",
    "    yy_true_train, yy_pred_P_train,\n",
    "    class_names=np.arange(n_classes),\n",
    "    cmap=plt.cm.RdYlGn, title=\"Training (Source)\"\n",
    ")\n",
    "\n",
    "\n",
    "# === Val ===\n",
    "xx_val, yy_true_val = dset_val(batch_size=dset_val.NN_xx, seed=0, sampling_strategy=\"true_random\", to_torch=True, device=device)\n",
    "\n",
    "with torch.no_grad():\n",
    "        features_val = model_encoder(xx_val)\n",
    "        logits = model_downstream(features_val)\n",
    "yy_pred_P_val = torch.nn.functional.softmax(logits, dim=1)\n",
    "yy_pred_P_val = yy_pred_P_val.cpu().numpy()\n",
    "yy_pred_val = np.argmax(yy_pred_P_val, axis=1)\n",
    "yy_true_val = yy_true_val.cpu().numpy()\n",
    "xx_val = xx_val.cpu().numpy()\n",
    "features_val = features_val.cpu().numpy()\n",
    "\n",
    "evaluation_tools.plot_confusion_matrix(\n",
    "    yy_true_val, yy_pred_P_val,\n",
    "    class_names=np.arange(n_classes),\n",
    "    cmap=plt.cm.RdYlGn, title=\"Validation (Source)\"\n",
    ")\n",
    "\n",
    "\n",
    "# === Test ===\n",
    "xx_test, yy_true_test = dset_test(batch_size=dset_test.NN_xx, seed=0, sampling_strategy=\"true_random\", to_torch=True, device=device)\n",
    "\n",
    "with torch.no_grad():\n",
    "        features_test = model_encoder(xx_test)\n",
    "        logits = model_downstream(features_test)\n",
    "yy_pred_P_test = torch.nn.functional.softmax(logits, dim=1)\n",
    "yy_pred_P_test = yy_pred_P_test.cpu().numpy()\n",
    "yy_pred_test = np.argmax(yy_pred_P_test, axis=1)\n",
    "yy_true_test = yy_true_test.cpu().numpy()\n",
    "xx_test = xx_test.cpu().numpy()\n",
    "features_test = features_test.cpu().numpy()\n",
    "\n",
    "evaluation_tools.plot_confusion_matrix(\n",
    "    yy_true_test, yy_pred_P_test,\n",
    "    class_names=np.arange(n_classes),\n",
    "    cmap=plt.cm.RdYlGn, title=\"Test (Target)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore latent space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_tools.plot_latents_scatter(\n",
    "    features_grid_points.cpu().numpy(),\n",
    "    yy_pred_grid_points,\n",
    "    class_counts=dset_test.class_counts,\n",
    "    class_names=None,\n",
    "    title=\"latents of grid points\",\n",
    "    n_bins=128,\n",
    "    sigma=2.0,\n",
    "    scatter_size=1.0,\n",
    "    scatter_alpha=1.0,\n",
    "    xlabel=\"Latent 1\",\n",
    "    ylabel=\"Latent 2\"\n",
    ")\n",
    "\n",
    "# === Perform shared t-SNE projection ===\n",
    "perplexity = 100\n",
    "tsne = TSNE(n_components=2, perplexity=perplexity, init='pca', random_state=42)\n",
    "X_features_grid_points_tsne = tsne.fit_transform(features_grid_points.cpu().numpy())\n",
    "evaluation_tools.plot_latents_scatter(\n",
    "    X_features_grid_points_tsne,\n",
    "    yy_pred_grid_points,\n",
    "    class_counts=dset_test.class_counts,\n",
    "    class_names=None,\n",
    "    title=\"latents of grid points: t-SNE with perplexity=\"+str(perplexity),\n",
    "    n_bins=128,\n",
    "    sigma=2.0,\n",
    "    scatter_size=1.0,\n",
    "    scatter_alpha=1.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_dict = {\n",
    "    \"latents_Source\": features_val,\n",
    "    \"latents_Target\": features_test\n",
    "}\n",
    "\n",
    "latents_tSNE = evaluation_tools.tsne_per_key(\n",
    "    feat_dict,\n",
    "    standardize=False,\n",
    "    subsample=None,\n",
    "    random_state=137,\n",
    "    tsne_kwargs={\"perplexity\": 100},\n",
    "    return_all_key=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xlim = (-100, 100)\n",
    "ylim = (-100, 100)\n",
    "\n",
    "evaluation_tools.plot_latents_scatter_val_test(\n",
    "    X_val=latents_tSNE[\"latents_Source_tSNE\"], y_val=yy_true_val,\n",
    "    X_test=latents_tSNE[\"latents_Target_tSNE\"], y_test=yy_true_test,\n",
    "    class_names=None,\n",
    "    title=\"Latents t-SNE ‚Äî Source vs Target\",\n",
    "    marker_val=\"o\", marker_test=\"^\",\n",
    "    size_val=14, size_test=14, alpha_val=0.7, alpha_test=0.7,\n",
    "    xlim=xlim, ylim=ylim,\n",
    "    subsample=4000, seed=137,\n",
    "    edgecolor=None, linewidths=0.0,\n",
    "    legend_split_1=\"Source (Validation)\",\n",
    "    legend_split_2=\"Target (Test)\"\n",
    ")\n",
    "\n",
    "evaluation_tools.plot_latents_scatter(\n",
    "    latents_tSNE[\"latents_Source_tSNE\"], yy_true_val,\n",
    "    class_counts=dset_val.class_counts,\n",
    "    class_names=None,\n",
    "    title=\"Latents t-SNE: Source\",\n",
    "    n_bins=128, sigma=2.0,\n",
    "    scatter_size=1.0, scatter_alpha=1.0,\n",
    "    xlim=xlim, ylim=ylim\n",
    ")\n",
    "evaluation_tools.plot_latent_density_2d(\n",
    "    latents_tSNE[\"latents_Source_tSNE\"],\n",
    "    title=\"Latents t-SNE: Source\",\n",
    "    density_method=\"hist\", # or \"kde\"\n",
    "    bins=256,\n",
    "    sigma=2.0, # ignored if density_method=\"kde\"\n",
    "    norm_mode=\"max\",\n",
    "    color_scale=\"linear\",\n",
    "    mask_zero_support=True,\n",
    "    contour_fracs=(0.01, 0.1, 0.3, 0.6),\n",
    "    contour_colors=\"k\",\n",
    "    contour_linewidths=0.4,\n",
    "    contour_label_fontsize=7,\n",
    "    contour_label_color=\"k\",\n",
    "    show_points=False,\n",
    "    points_alpha=0.1,\n",
    "    points_size=2,\n",
    "    random_subsample=None,\n",
    "    xlim=xlim,\n",
    "    ylim=ylim\n",
    ")\n",
    "\n",
    "evaluation_tools.plot_latents_scatter(\n",
    "    latents_tSNE[\"latents_Target_tSNE\"], yy_true_test,\n",
    "    class_counts=dset_test.class_counts,\n",
    "    class_names=None,\n",
    "    title=\"Latents t-SNE: Target\",\n",
    "    n_bins=128, sigma=2.0,\n",
    "    scatter_size=1.0, scatter_alpha=1.0,\n",
    "    xlim=xlim, ylim=ylim\n",
    ")\n",
    "evaluation_tools.plot_latent_density_2d(\n",
    "    latents_tSNE[\"latents_Target_tSNE\"],\n",
    "    title=\"Latents t-SNE: Target\",\n",
    "    density_method=\"hist\", # or \"kde\"\n",
    "    bins=256,\n",
    "    sigma=2.0, # ignored if density_method=\"kde\"\n",
    "    norm_mode=\"max\",\n",
    "    color_scale=\"linear\",\n",
    "    mask_zero_support=True,\n",
    "    contour_fracs=(0.01, 0.1, 0.3, 0.6),\n",
    "    contour_colors=\"k\",\n",
    "    contour_linewidths=0.4,\n",
    "    contour_label_fontsize=7,\n",
    "    contour_label_color=\"k\",\n",
    "    show_points=False,\n",
    "    points_alpha=0.1,\n",
    "    points_size=2,\n",
    "    random_subsample=None,\n",
    "    xlim=xlim,\n",
    "    ylim=ylim\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
