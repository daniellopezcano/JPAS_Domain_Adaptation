{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s') # NOTSET, DEBUG, INFO, WARN, ERROR, CRITICAL\n",
    "\n",
    "from JPAS_DA import global_setup\n",
    "from JPAS_DA.data import data_loaders\n",
    "from JPAS_DA.data import generate_toy_data\n",
    "from JPAS_DA.models import model_building_tools\n",
    "from JPAS_DA.training import training_tools\n",
    "from JPAS_DA.training import save_load_tools\n",
    "from JPAS_DA.evaluation import evaluation_tools\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "from JPAS_DA.utils import plotting_utils\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('default')\n",
    "plt.close('all')\n",
    "font, rcnew = plotting_utils.matplotlib_default_config()\n",
    "mpl.rc('font', **font)\n",
    "plt.rcParams.update(rcnew)\n",
    "plt.style.use('tableau-colorblind10')\n",
    "%matplotlib widget\n",
    "\n",
    "from JPAS_DA.utils import aux_tools\n",
    "aux_tools.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Shared Parameters ===\n",
    "n_classes = 6\n",
    "n_features = 2\n",
    "class_center_range = [(-10, 10), (-10, 10)]\n",
    "cov_scale_range = (0.1, 1.1)\n",
    "class_proportions = np.array([0.55, 0.05, 0.15, 0.05, 0.1, 0.1])\n",
    "assert np.isclose(class_proportions.sum(), 1.0)\n",
    "\n",
    "# === Sample Sizes and Seeds ===\n",
    "n_samples_train = 10000\n",
    "n_samples_val = 4000\n",
    "n_samples_test = 4000\n",
    "n_samples_train_DA = 4000\n",
    "n_samples_val_DA = 4000\n",
    "\n",
    "seed_structure = 1     # Used to define centers and covariances\n",
    "seed_train = 42        # Ensures different train/val/test samples\n",
    "seed_val = 13\n",
    "seed_test = 0\n",
    "seed_train_DA = 2\n",
    "seed_val_DA = 3\n",
    "seed_shift = 4\n",
    "\n",
    "# === Generate fixed shared structure for train/val ===\n",
    "shared_centers, shared_covs = generate_toy_data.generate_centers_covs(n_classes, n_features, class_center_range, cov_scale_range, seed_structure)\n",
    "\n",
    "# === Generate train/val sets with different seeds but same structure ===\n",
    "xx_train, yy_train, train_counts = generate_toy_data.generate_dataset_from_structure(n_samples_train, shared_centers, shared_covs, class_proportions, seed=seed_train)\n",
    "xx_val, yy_val, val_counts = generate_toy_data.generate_dataset_from_structure(n_samples_val, shared_centers, shared_covs, class_proportions, seed=seed_val)\n",
    "\n",
    "# === Generate test set with shifted centers and covs ===\n",
    "shifted_centers, shifted_covs = generate_toy_data.shift_centers_covs(shared_centers, shared_covs, center_shift=1.0, cov_shift=0.5, seed=seed_shift)\n",
    "\n",
    "shifted_centers = np.array([\n",
    "[-1.65955991,  4.40648987],\n",
    "[ 2.50439001, -2.81095501],\n",
    "[ 6.01489137,  9.36523151],\n",
    "[ 5.6633057 ,  7.83754228],\n",
    "[-2.43547987, -1.04212948],\n",
    "[ 3.57671066, -5.76743768]\n",
    "])\n",
    "xx_test, yy_test, test_counts = generate_toy_data.generate_dataset_from_structure(n_samples_test, shifted_centers, shifted_covs, class_proportions, seed=seed_test)\n",
    "xx_train_DA, yy_train_DA, _ = generate_toy_data.generate_dataset_from_structure(n_samples_val_DA, shifted_centers, shifted_covs, class_proportions, seed=seed_val_DA)\n",
    "xx_val_DA, yy_val_DA, _ = generate_toy_data.generate_dataset_from_structure(n_samples_val_DA, shifted_centers, shifted_covs, class_proportions, seed=seed_val_DA)\n",
    "\n",
    "\n",
    "# === Visualize sets ===\n",
    "color_dict = {cls: plt.cm.get_cmap(\"tab10\")(i) for i, cls in enumerate(np.arange(n_classes))}\n",
    "\n",
    "fig1, ax_main1 = plotting_utils.plot_2d_classification_with_kde(xx_train[\"OBS\"], yy_train[\"SPECTYPE_int\"], title=\"Train Set\", class_color_dict=color_dict)\n",
    "for i in range(n_classes):\n",
    "    ax_main1.scatter(shared_centers[i, 0], shared_centers[i, 1], marker=\"x\", color='black', s=160, linewidth=4, zorder=1)\n",
    "    ax_main1.scatter(shared_centers[i, 0], shared_centers[i, 1], marker=\"x\", color=color_dict[i], s=100, linewidth=2, zorder=2)\n",
    "ax_main1.set_xlim(-15, 15)\n",
    "ax_main1.set_ylim(-15, 15)\n",
    "plt.show()\n",
    "\n",
    "fig2, ax_main2 = plotting_utils.plot_2d_classification_with_kde(xx_val[\"OBS\"], yy_val[\"SPECTYPE_int\"], title=\"Validation Set\", class_color_dict=color_dict)\n",
    "for i in range(n_classes):\n",
    "    ax_main2.scatter(shared_centers[i, 0], shared_centers[i, 1], marker=\"x\", color='black', s=160, linewidth=4, zorder=1)\n",
    "    ax_main2.scatter(shared_centers[i, 0], shared_centers[i, 1], marker=\"x\", color=color_dict[i], s=100, linewidth=2, zorder=2)\n",
    "ax_main2.set_xlim(-15, 15)\n",
    "ax_main2.set_ylim(-15, 15)\n",
    "plt.show()\n",
    "\n",
    "fig3, ax_main3 = plotting_utils.plot_2d_classification_with_kde(xx_test[\"OBS\"], yy_test[\"SPECTYPE_int\"], title=\"Test Set\", class_color_dict=color_dict)\n",
    "for i in range(n_classes):\n",
    "    ax_main3.scatter(shared_centers[i, 0], shared_centers[i, 1], marker=\"x\", color='black', s=160, linewidth=4, zorder=1)\n",
    "    ax_main3.scatter(shared_centers[i, 0], shared_centers[i, 1], marker=\"x\", color=color_dict[i], s=100, linewidth=2, zorder=2)\n",
    "for i in range(n_classes):\n",
    "    ax_main3.scatter(shifted_centers[i, 0], shifted_centers[i, 1], marker=\"x\", color='black', s=160, linewidth=4, zorder=1)\n",
    "    ax_main3.scatter(shifted_centers[i, 0], shifted_centers[i, 1], marker=\"x\", color=color_dict[i], s=100, linewidth=2, zorder=2)\n",
    "for i in range(n_classes):\n",
    "    delta = shifted_centers[i] - shared_centers[i]\n",
    "    ax_main3.arrow(shared_centers[i, 0], shared_centers[i, 1], delta[0], delta[1], color=color_dict[i], linewidth=2, head_width=0.2, length_includes_head=True)\n",
    "ax_main3.set_xlim(-15, 15)\n",
    "ax_main3.set_ylim(-15, 15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_load = os.path.join(global_setup.path_models, \"06_example_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if path_load is None:\n",
    "\n",
    "    dset_train = data_loaders.DataLoader(xx_train, yy_train, normalize=True, provided_normalization=None)\n",
    "\n",
    "    means = dset_train.means\n",
    "    stds = dset_train.stds\n",
    "    \n",
    "    dset_val = data_loaders.DataLoader(xx_val, yy_val, normalize=True, provided_normalization=(means, stds))\n",
    "    dset_test = data_loaders.DataLoader(xx_test, yy_test, normalize=True, provided_normalization=(means, stds))\n",
    "\n",
    "else:\n",
    "    means, stds = save_load_tools.load_means_stds(path_load)\n",
    "\n",
    "    dset_train = data_loaders.DataLoader(xx_train_DA, yy_train_DA, normalize=True, provided_normalization=(means, stds))\n",
    "    dset_val = data_loaders.DataLoader(xx_val_DA, yy_val_DA, normalize=True, provided_normalization=(means, stds))\n",
    "    dset_test = data_loaders.DataLoader(xx_test, yy_test, normalize=True, provided_normalization=(means, stds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_load_encoder = os.path.join(path_load, \"model_encoder.pt\") if path_load else None\n",
    "path_load_downstream = os.path.join(path_load, \"model_downstream.pt\") if path_load else None\n",
    "\n",
    "# hidden_layers_encoder = [32, 16, 16]\n",
    "# dropout_rates_encoder = [0.01, 0.01, 0.01]\n",
    "# output_dim_encoder = 2\n",
    "\n",
    "# hidden_layers_downstream = [8, 8, 8]\n",
    "# dropout_rates_downstream = [0.01, 0.01, 0.01]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if path_load_encoder:\n",
    "    assert os.path.isfile(path_load_encoder), f\"File does not exist: {path_load_encoder}\"\n",
    "    config_encoder, model_encoder = save_load_tools.load_model_from_checkpoint(\n",
    "        path_load_encoder, model_building_tools.create_mlp\n",
    "    )\n",
    "else:\n",
    "    xx, _ = dset_train(batch_size=1)\n",
    "    config_encoder = {\n",
    "        'input_dim': xx.shape[-1],\n",
    "        'hidden_layers': hidden_layers_encoder,\n",
    "        'dropout_rates': dropout_rates_encoder,\n",
    "        'output_dim': output_dim_encoder,\n",
    "        'use_batchnorm': False,\n",
    "        'use_layernorm_at_output': False,\n",
    "        'init_method': 'xavier'\n",
    "    }\n",
    "    model_encoder = model_building_tools.create_mlp(**config_encoder)\n",
    "\n",
    "if path_load_downstream:\n",
    "    assert os.path.isfile(path_load_downstream), f\"File does not exist: {path_load_downstream}\"\n",
    "    config_downstream, model_downstream = save_load_tools.load_model_from_checkpoint(\n",
    "        path_load_downstream, model_building_tools.create_mlp\n",
    "    )\n",
    "else:\n",
    "    config_downstream = {\n",
    "        'input_dim': output_dim_encoder,\n",
    "        'hidden_layers': hidden_layers_downstream,\n",
    "        'dropout_rates': dropout_rates_downstream,\n",
    "        'output_dim': n_classes,\n",
    "        'use_batchnorm': False,\n",
    "        'use_layernorm_at_output': False,\n",
    "        'init_method': 'xavier'\n",
    "    }\n",
    "    model_downstream = model_building_tools.create_mlp(**config_downstream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_encoder)\n",
    "print(model_downstream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_strategy = \"true_random\"\n",
    "freeze_downstream_model = True\n",
    "\n",
    "path_save = '/home/dlopez/Documentos/0.profesional/Postdoc/USP/Projects/JPAS_Domain_Adaptation/SAVED_models'\n",
    "path_save += \"/07_example_model_DA\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if sampling_strategy == \"true_random\":\n",
    "    counts = dset_train.class_counts\n",
    "    total_samples = np.sum(counts)\n",
    "    weights = total_samples / (n_classes * counts)\n",
    "    class_weights = torch.tensor(weights, dtype=torch.float32)\n",
    "if sampling_strategy == \"class_random\":\n",
    "    class_weights = torch.tensor(np.ones(n_classes), dtype=torch.float32)\n",
    "\n",
    "loss_function_dict = {\n",
    "    \"type\": \"CrossEntropyLoss\",\n",
    "    \"sampling_strategy\": sampling_strategy,\n",
    "    \"class_weights\": class_weights\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_val_loss = training_tools.train_model(\n",
    "    dset_train=dset_train,\n",
    "    model_encoder=model_encoder,\n",
    "    model_downstream=model_downstream,\n",
    "    loss_function_dict=loss_function_dict,\n",
    "    freeze_downstream_model=freeze_downstream_model,\n",
    "    dset_val=dset_val,\n",
    "    NN_epochs=300,\n",
    "    NN_batches_per_epoch=32,\n",
    "    batch_size=128,\n",
    "    batch_size_val=len(dset_val.yy[list(dset_val.yy.keys())[0]]),\n",
    "    lr=0.005,\n",
    "    weight_decay=0.001,\n",
    "    clip_grad_norm=10.0,\n",
    "    seed_mode=\"deterministic\",\n",
    "    seed=0,\n",
    "    path_save=path_save,\n",
    "    config_encoder=config_encoder,\n",
    "    config_downstream=config_downstream\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting_utils.plot_training_curves(path_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = next(model_encoder.parameters()).device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_labels = dset_train.class_labels\n",
    "\n",
    "# === Generate meshgrid from training data bounds ===\n",
    "x_min, x_max = -15, 15\n",
    "y_min, y_max = -15, 15\n",
    "\n",
    "grid_res = 256\n",
    "xx_vals = np.linspace(x_min, x_max, grid_res)\n",
    "yy_vals = np.linspace(y_min, y_max, grid_res)\n",
    "xx_mesh, yy_mesh = np.meshgrid(xx_vals, yy_vals)\n",
    "\n",
    "grid_points = np.stack([xx_mesh.ravel(), yy_mesh.ravel()], axis=1)\n",
    "grid_points = (grid_points - dset_train.means[0]) / dset_train.stds[0]\n",
    "xx_grid = torch.tensor(grid_points, dtype=torch.float32, device=device)\n",
    "\n",
    "# === Compute class probabilities and predicted class ===\n",
    "with torch.no_grad():\n",
    "    features = model_encoder(xx_grid)\n",
    "    logits = model_downstream(features)\n",
    "    yy_pred_P = torch.nn.functional.softmax(logits, dim=1).cpu().numpy()\n",
    "    yy_pred = np.argmax(yy_pred_P, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Reshape to grid ===\n",
    "Z = yy_pred.reshape(grid_res, grid_res).astype(float)  # cast to float for gouraud shading\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 7))\n",
    "\n",
    "# Discrete colormap (but visually smoothed by gouraud)\n",
    "unique_labels = np.unique(Z.astype(int))\n",
    "cmap_base = plt.cm.get_cmap(\"tab10\")\n",
    "class_color_dict = {label: cmap_base(label) for label in unique_labels}\n",
    "colors = [class_color_dict[label] for label in unique_labels]\n",
    "cmap = mpl.colors.ListedColormap(colors)\n",
    "\n",
    "# Use boundaries to align ticks to color blocks\n",
    "boundaries = np.arange(len(unique_labels) + 1) - 0.5\n",
    "norm = mpl.colors.BoundaryNorm(boundaries, ncolors=len(unique_labels))\n",
    "pcm = ax.pcolormesh(xx_mesh, yy_mesh, Z, cmap=cmap, norm=norm, shading='nearest')\n",
    "\n",
    "# Overlay training points using the same colormap\n",
    "for ii, label in enumerate(unique_labels):\n",
    "    mask = dset_train.yy[\"SPECTYPE_int\"] == label\n",
    "    tmp = (dset_train.xx[\"OBS\"][mask] * dset_train.stds[0]) + dset_train.means[0]\n",
    "    ax.scatter(tmp[:, 0], tmp[:, 1], s=25, alpha=0.9, color=class_color_dict[label],\n",
    "               edgecolor='black', linewidth=0.4)\n",
    "\n",
    "# Styling\n",
    "ax.set_title(\"Predicted Class Map\", fontsize=18)\n",
    "ax.set_xlabel(r\"$\\mathrm{Feature~1}$\", fontsize=16)\n",
    "ax.set_ylabel(r\"$\\mathrm{Feature~2}$\", fontsize=16)\n",
    "ax.tick_params(labelsize=12)\n",
    "\n",
    "# Colorbar (optional: no strict boundaries here due to gouraud shading)\n",
    "cbar = fig.colorbar(pcm, ax=ax, orientation='vertical', fraction=0.046, pad=0.04)\n",
    "cbar.set_label(\"Predicted Class\", fontsize=14)\n",
    "cbar.ax.tick_params(labelsize=12)\n",
    "cbar.ax.set_yticks(np.arange(len(unique_labels)))\n",
    "cbar.ax.set_yticklabels(unique_labels)\n",
    "\n",
    "ax.set_xlim(x_min, x_max)\n",
    "ax.set_ylim(y_min, y_max)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Reshape to grid ===\n",
    "Z = yy_pred_P.reshape(grid_res, grid_res, n_classes)\n",
    "\n",
    "# Determine a roughly square grid layout\n",
    "n_cols = np.ceil(np.sqrt(n_classes)).astype(int)\n",
    "n_rows = np.ceil(n_classes / n_cols).astype(int)\n",
    "\n",
    "fig, axs = plt.subplots(n_rows, n_cols, figsize=(4.5 * n_cols, 4.5 * n_rows), sharex=True, sharey=True)\n",
    "axs = np.array(axs).reshape(n_rows, n_cols)  # Ensure 2D array for indexing\n",
    "\n",
    "vmin, vmax = 0.0, 1.0\n",
    "cmap = \"seismic\"\n",
    "pcm = None\n",
    "\n",
    "for idx in range(n_classes):\n",
    "    row, col = divmod(idx, n_cols)\n",
    "    ax = axs[row, col]\n",
    "\n",
    "    pcm = ax.pcolormesh(xx_mesh, yy_mesh, Z[..., idx], cmap=cmap, shading='auto', vmin=vmin, vmax=vmax)\n",
    "\n",
    "    # Annotate class index\n",
    "    ax.text(\n",
    "        0.98, 0.98, f\"Class {idx}\", transform=ax.transAxes, ha='right', va='top', fontsize=14, fontweight='bold',\n",
    "        bbox=dict(facecolor='white', edgecolor='black', boxstyle='round,pad=0.3', alpha=0.85)\n",
    "    )\n",
    "\n",
    "    ax.set_xlabel(r\"$\\mathrm{Feature~1}$\", fontsize=16)\n",
    "    ax.set_ylabel(r\"$\\mathrm{Feature~2}$\", fontsize=16)\n",
    "    ax.tick_params(axis='both', labelsize=14)\n",
    "\n",
    "    for jj, label in enumerate(unique_labels):\n",
    "        mask = dset_train.yy[\"SPECTYPE_int\"] == label\n",
    "        tmp = (dset_train.xx[\"OBS\"][mask] * dset_train.stds[0]) + dset_train.means[0]\n",
    "        color = 'limegreen' if idx == jj else 'k'\n",
    "        ax.scatter(tmp[:, 0], tmp[:, 1], s=12, alpha=0.4, color=color)\n",
    "\n",
    "    ax.set_xlim(x_min, x_max)\n",
    "    ax.set_ylim(y_min, y_max)\n",
    "    \n",
    "# Hide unused axes\n",
    "for idx in range(n_classes, n_rows * n_cols):\n",
    "    row, col = divmod(idx, n_cols)\n",
    "    axs[row, col].axis(\"off\")\n",
    "\n",
    "# Adjust spacing between subplots\n",
    "plt.subplots_adjust(hspace=0.3, wspace=0.3)\n",
    "\n",
    "# === Shared horizontal colorbar at the top ===\n",
    "pos0 = axs[0, 0].get_position()\n",
    "posn = axs[-1, -1 if n_classes % n_cols != 0 else n_cols - 1].get_position()\n",
    "cbar_ax = fig.add_axes([pos0.x0, pos0.y1 + 0.04, posn.x1 - pos0.x0, 0.02])\n",
    "cbar = fig.colorbar(pcm, cax=cbar_ax, orientation='horizontal')\n",
    "cbar.ax.set_title(\"Predicted Class Probability\", fontsize=16, pad=8)\n",
    "cbar.ax.tick_params(labelsize=14)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Train ===\n",
    "xx_train, yy_true_train = dset_train(batch_size=dset_train.NN_xx, seed=0, sampling_strategy=\"true_random\", to_torch=True, device=device)\n",
    "\n",
    "with torch.no_grad():\n",
    "        features_train = model_encoder(xx_train)\n",
    "        logits = model_downstream(features_train)\n",
    "yy_pred_P_train = torch.nn.functional.softmax(logits, dim=1)\n",
    "yy_pred_P_train = yy_pred_P_train.cpu().numpy()\n",
    "yy_pred_train = np.argmax(yy_pred_P_train, axis=1)\n",
    "yy_true_train = yy_true_train.cpu().numpy()\n",
    "xx_train = xx_train.cpu().numpy()\n",
    "features_train = features_train.cpu().numpy()\n",
    "\n",
    "evaluation_tools.plot_confusion_matrix(\n",
    "    yy_true_train, yy_pred_P_train,\n",
    "    class_names=np.arange(n_classes),\n",
    "    cmap=plt.cm.RdYlGn, title=\"train\"\n",
    ")\n",
    "\n",
    "\n",
    "# === Val ===\n",
    "xx_val, yy_true_val = dset_val(batch_size=dset_val.NN_xx, seed=0, sampling_strategy=\"true_random\", to_torch=True, device=device)\n",
    "\n",
    "with torch.no_grad():\n",
    "        features_val = model_encoder(xx_val)\n",
    "        logits = model_downstream(features_val)\n",
    "yy_pred_P_val = torch.nn.functional.softmax(logits, dim=1)\n",
    "yy_pred_P_val = yy_pred_P_val.cpu().numpy()\n",
    "yy_pred_val = np.argmax(yy_pred_P_val, axis=1)\n",
    "yy_true_val = yy_true_val.cpu().numpy()\n",
    "xx_val = xx_val.cpu().numpy()\n",
    "features_val = features_val.cpu().numpy()\n",
    "\n",
    "evaluation_tools.plot_confusion_matrix(\n",
    "    yy_true_val, yy_pred_P_val,\n",
    "    class_names=np.arange(n_classes),\n",
    "    cmap=plt.cm.RdYlGn, title=\"val\"\n",
    ")\n",
    "\n",
    "\n",
    "# === Test ===\n",
    "xx_test, yy_true_test = dset_test(batch_size=dset_test.NN_xx, seed=0, sampling_strategy=\"true_random\", to_torch=True, device=device)\n",
    "\n",
    "with torch.no_grad():\n",
    "        features_test = model_encoder(xx_test)\n",
    "        logits = model_downstream(features_test)\n",
    "yy_pred_P_test = torch.nn.functional.softmax(logits, dim=1)\n",
    "yy_pred_P_test = yy_pred_P_test.cpu().numpy()\n",
    "yy_pred_test = np.argmax(yy_pred_P_test, axis=1)\n",
    "yy_true_test = yy_true_test.cpu().numpy()\n",
    "xx_test = xx_test.cpu().numpy()\n",
    "features_test = features_test.cpu().numpy()\n",
    "\n",
    "evaluation_tools.plot_confusion_matrix(\n",
    "    yy_true_test, yy_pred_P_test,\n",
    "    class_names=np.arange(n_classes),\n",
    "    cmap=plt.cm.RdYlGn, title=\"test\"\n",
    ")\n",
    "\n",
    "\n",
    "# === Compare Val VS Test ===\n",
    "evaluation_tools.compare_TPR_confusion_matrices(\n",
    "    yy_true_val,\n",
    "    yy_pred_P_val,\n",
    "    yy_true_test,\n",
    "    yy_pred_P_test,\n",
    "    class_names=np.arange(n_classes),\n",
    "    figsize=(10, 7),\n",
    "    cmap='seismic',\n",
    "    title='TPR Comparison: Test vs Validation'\n",
    ")\n",
    "\n",
    "metrics = evaluation_tools.compare_sets_performance(\n",
    "    yy_true_val, yy_pred_P_val,\n",
    "    yy_true_test, yy_pred_P_test,\n",
    "    class_names=np.arange(n_classes),\n",
    "    name_1=\"val\",\n",
    "    name_2=\"test\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# === Stack all feature representations together ===\n",
    "n_val = features_val.shape[0]\n",
    "n_test = features_test.shape[0]\n",
    "\n",
    "X_all = np.vstack([\n",
    "    features_val,\n",
    "    features_test\n",
    "])\n",
    "\n",
    "# === Perform shared t-SNE projection ===\n",
    "tsne = TSNE(n_components=2, perplexity=30, init='pca', random_state=42)\n",
    "X_all_tsne = tsne.fit_transform(X_all)\n",
    "\n",
    "# === Split back to original domains ===\n",
    "i0 = 0\n",
    "i1 = i0 + n_val\n",
    "i2 = i1 + n_test\n",
    "\n",
    "X_val   = X_all_tsne[i0:i1]\n",
    "X_test      = X_all_tsne[i1:i2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_tools.plot_tsne_comparison_single_pair(\n",
    "    X_val, yy_true_val,\n",
    "    X_test, yy_true_test,\n",
    "    dset_test.class_counts,\n",
    "    class_names=None,\n",
    "    title_set1=\"Validation\",\n",
    "    title_set2=\"Test\",\n",
    "    n_bins=128,\n",
    "    sigma=2.0,\n",
    "    scatter_size=1,\n",
    "    scatter_alpha=1.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
