{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s') # NOTSET, DEBUG, INFO, WARN, ERROR, CRITICAL\n",
    "\n",
    "from JPAS_DA import global_setup\n",
    "\n",
    "from JPAS_DA.data import loading_tools\n",
    "from JPAS_DA.data import cleaning_tools\n",
    "from JPAS_DA.data import crossmatch_tools\n",
    "from JPAS_DA.data import process_dset_splits\n",
    "from JPAS_DA.data import wrapper_data_loaders\n",
    "\n",
    "from JPAS_DA.models import model_building_tools\n",
    "from JPAS_DA.training import save_load_tools\n",
    "from JPAS_DA.evaluation import evaluation_tools\n",
    "from JPAS_DA.wrapper_wandb import wrapper_tools\n",
    "from JPAS_DA.evaluation import evaluation_tools\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "from JPAS_DA.utils import plotting_utils\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('default')\n",
    "plt.close('all')\n",
    "font, rcnew = plotting_utils.matplotlib_default_config()\n",
    "mpl.rc('font', **font)\n",
    "plt.rcParams.update(rcnew)\n",
    "plt.style.use('tableau-colorblind10')\n",
    "%matplotlib widget\n",
    "\n",
    "from JPAS_DA.utils import aux_tools\n",
    "aux_tools.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_load_no_DA = \"09_no_DA\"\n",
    "path_load_DA = \"09_DA\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, model_encoder_no_DA = save_load_tools.load_model_from_checkpoint(os.path.join(global_setup.path_models, path_load_no_DA, \"model_encoder.pt\"), model_building_tools.create_mlp)\n",
    "_, model_downstream_no_DA = save_load_tools.load_model_from_checkpoint(os.path.join(global_setup.path_models, path_load_no_DA, \"model_downstream.pt\"), model_building_tools.create_mlp)\n",
    "\n",
    "_, model_encoder_DA = save_load_tools.load_model_from_checkpoint(os.path.join(global_setup.path_models, path_load_DA, \"model_encoder.pt\"), model_building_tools.create_mlp)\n",
    "_, model_downstream_DA = save_load_tools.load_model_from_checkpoint(os.path.join(global_setup.path_models, path_load_DA, \"model_downstream.pt\"), model_building_tools.create_mlp)\n",
    "\n",
    "_ = evaluation_tools.compare_model_parameters(model_downstream_no_DA, model_downstream_DA, rtol=1e-2, atol=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, config_no_DA = wrapper_tools.load_and_massage_config_file(os.path.join(global_setup.path_models, path_load_no_DA, \"config.yaml\"), path_load_no_DA)\n",
    "_, config_DA = wrapper_tools.load_and_massage_config_file(os.path.join(global_setup.path_models, path_load_DA, \"config.yaml\"), path_load_DA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_data = config_DA[\"data\"]\n",
    "\n",
    "tmp_key = \"data_paths\"\n",
    "root_path = config_data[tmp_key][\"root_path\"]\n",
    "load_JPAS_data = config_data[tmp_key][\"load_JPAS_data\"]\n",
    "load_DESI_data = config_data[tmp_key][\"load_DESI_data\"]\n",
    "random_seed_load = config_data[tmp_key][\"random_seed_load\"]\n",
    "\n",
    "tmp_key = \"dict_clean_data_options\"\n",
    "apply_masks = config_data[tmp_key][\"apply_masks\"]\n",
    "mask_indices = config_data[tmp_key][\"mask_indices\"]\n",
    "magic_numbers = config_data[tmp_key][\"magic_numbers\"]\n",
    "i_band_sn_threshold = config_data[tmp_key][\"i_band_sn_threshold\"]\n",
    "z_lim_QSO_cut = config_data[tmp_key][\"z_lim_QSO_cut\"]\n",
    "\n",
    "tmp_key = \"dict_split_data_options\"\n",
    "train_ratio_both = config_data[tmp_key][\"train_ratio_both\"]\n",
    "val_ratio_both = config_data[tmp_key][\"val_ratio_both\"]\n",
    "test_ratio_both = config_data[tmp_key][\"test_ratio_both\"]\n",
    "random_seed_split_both = config_data[tmp_key][\"random_seed_split_both\"]\n",
    "train_ratio_only_DESI = config_data[tmp_key][\"train_ratio_only_DESI\"]\n",
    "val_ratio_only_DESI = config_data[tmp_key][\"val_ratio_only_DESI\"]\n",
    "test_ratio_only_DESI = config_data[tmp_key][\"test_ratio_only_DESI\"]\n",
    "random_seed_split_only_DESI = config_data[tmp_key][\"random_seed_split_only_DESI\"]\n",
    "\n",
    "define_dataset_loaders_keys = ['DESI_only', \"JPAS_matched\"]\n",
    "keys_xx = config_data[\"features_labels_options\"][\"keys_xx\"]\n",
    "keys_yy = [\"SPECTYPE_int\", \"TARGETID\", \"DESI_FLUX_R\"]\n",
    "normalize = True\n",
    "provided_normalization = config_data[\"provided_normalization\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ───────────────────────────────────────────────────── #\n",
    "# 1. Load raw JPAS and DESI datasets\n",
    "# ───────────────────────────────────────────────────── #\n",
    "logging.info(\"\\n\\n1️⃣: Loading datasets from disk...\")\n",
    "DATA = loading_tools.load_dsets(\n",
    "    root_path=root_path,\n",
    "    datasets_jpas=load_JPAS_data,\n",
    "    datasets_desi=load_DESI_data,\n",
    "    random_seed=random_seed_load\n",
    ")\n",
    "\n",
    "# ───────────────────────────────────────────────────── #\n",
    "# 2. Apply cleaning and masking procedures\n",
    "# ───────────────────────────────────────────────────── #\n",
    "logging.info(\"\\n\\n2️⃣: Cleaning and masking data...\")\n",
    "DATA = cleaning_tools.clean_and_mask_data(\n",
    "    DATA=DATA,\n",
    "    apply_masks=apply_masks,\n",
    "    mask_indices=mask_indices,\n",
    "    magic_numbers=magic_numbers,\n",
    "    i_band_sn_threshold=i_band_sn_threshold,\n",
    "    z_lim_QSO_cut=z_lim_QSO_cut\n",
    ")\n",
    "\n",
    "# ───────────────────────────────────────────────────── #\n",
    "# 3. Crossmatch JPAS and DESI using TARGETID\n",
    "# ───────────────────────────────────────────────────── #\n",
    "logging.info(\"\\n\\n3️⃣: Crossmatching JPAS and DESI TARGETIDs...\")\n",
    "Dict_LoA = {\"both\": {}, \"only\": {}}\n",
    "IDs_only_DESI, IDs_only_JPAS, IDs_both, \\\n",
    "Dict_LoA[\"only\"][\"DESI\"], Dict_LoA[\"only\"][\"JPAS\"], \\\n",
    "Dict_LoA[\"both\"][\"DESI\"], Dict_LoA[\"both\"][\"JPAS\"] = crossmatch_tools.crossmatch_IDs_two_datasets(\n",
    "    DATA[\"DESI\"]['TARGETID'], DATA[\"JPAS\"]['TARGETID']\n",
    ")\n",
    "\n",
    "# ───────────────────────────────────────────────────── #\n",
    "# 4. Perform train/val/test splits\n",
    "# ───────────────────────────────────────────────────── #\n",
    "logging.info(\"\\n\\n4️⃣: Splitting data into train/val/test...\")\n",
    "Dict_LoA_split = {\"both\": {}, \"only\": {}}\n",
    "\n",
    "Dict_LoA_split[\"both\"][\"JPAS\"] = process_dset_splits.split_LoA(\n",
    "    Dict_LoA[\"both\"][\"JPAS\"], train_ratio_both, val_ratio_both, test_ratio_both, seed=random_seed_split_both\n",
    ")\n",
    "Dict_LoA_split[\"both\"][\"DESI\"] = process_dset_splits.split_LoA(\n",
    "    Dict_LoA[\"both\"][\"DESI\"], train_ratio_both, val_ratio_both, test_ratio_both, seed=random_seed_split_both\n",
    ")\n",
    "Dict_LoA_split[\"only\"][\"DESI\"] = process_dset_splits.split_LoA(\n",
    "    Dict_LoA[\"only\"][\"DESI\"], train_ratio_only_DESI, val_ratio_only_DESI, test_ratio_only_DESI, seed=random_seed_split_only_DESI\n",
    ")\n",
    "\n",
    "# ───────────────────────────────────────────────────── #\n",
    "# 5. Load data\n",
    "# ───────────────────────────────────────────────────── #\n",
    "logging.info(\"\\n\\n5️⃣: Load and normalize data...\")\n",
    "\n",
    "xx_dict = {}\n",
    "yy_dict = {}\n",
    "for key_dset in [\"val\", \"test\"]:\n",
    "    xx_dict[key_dset] = {}\n",
    "    yy_dict[key_dset] = {}\n",
    "    logging.info(f\"⚙️ Preparing split: {key_dset}\")\n",
    "    for key_loader in define_dataset_loaders_keys:\n",
    "        logging.info(f\"├── {key_loader}\")\n",
    "        if key_loader == \"DESI_combined\":\n",
    "            LoA, xx, yy = process_dset_splits.extract_and_combine_DESI_data(\n",
    "                Dict_LoA_split[\"only\"][\"DESI\"][key_dset], Dict_LoA_split[\"both\"][\"DESI\"][key_dset], DATA[\"DESI\"], keys_xx, keys_yy\n",
    "            )\n",
    "        elif key_loader == \"DESI_only\":\n",
    "            LoA, xx, yy = process_dset_splits.extract_data_using_LoA(\n",
    "                Dict_LoA_split[\"only\"][\"DESI\"][key_dset], DATA[\"DESI\"], keys_xx, keys_yy\n",
    "            )\n",
    "        elif key_loader == \"DESI_matched\":\n",
    "            LoA, xx, yy = process_dset_splits.extract_data_using_LoA(\n",
    "                Dict_LoA_split[\"both\"][\"DESI\"][key_dset], DATA[\"DESI\"], keys_xx, keys_yy\n",
    "            )\n",
    "        elif key_loader == \"JPAS_matched\":\n",
    "            LoA, xx, yy = process_dset_splits.extract_data_using_LoA(\n",
    "                Dict_LoA_split[\"both\"][\"JPAS\"][key_dset], DATA[\"JPAS\"], keys_xx, keys_yy\n",
    "            )\n",
    "        # Normalize, reshape, and stack all features in one pass\n",
    "        xx_stacked = np.concatenate([\n",
    "            np.atleast_2d((xx[kk] - provided_normalization[0][ii]) / provided_normalization[1][ii]).reshape(xx[kk].shape[0], -1)\n",
    "            for ii, kk in enumerate(xx)\n",
    "        ], axis=1)\n",
    "\n",
    "        # Store as torch tensor\n",
    "        xx_dict[key_dset][key_loader] = torch.tensor(xx_stacked, dtype=torch.float32, device=\"cpu\")\n",
    "        yy_dict[key_dset][key_loader] = yy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the validation set results for the networks trained without domain adaptation (employing the DESI mock spectra)\n",
    "key_dset = \"val\"\n",
    "key_loader = \"DESI_only\"\n",
    "\n",
    "tmp_xx = xx_dict[key_dset][key_loader]\n",
    "with torch.no_grad():\n",
    "    tmp_features = model_encoder_no_DA(tmp_xx)\n",
    "    tmp_logits = model_downstream_no_DA(tmp_features)\n",
    "tmp_yy_pred_P = torch.nn.functional.softmax(tmp_logits, dim=1).cpu().numpy()\n",
    "\n",
    "yy_dict[key_dset][key_loader]['no_DA_features'] = tmp_features.cpu().numpy()\n",
    "yy_dict[key_dset][key_loader]['no_DA_pred_Probabilities'] = tmp_yy_pred_P\n",
    "yy_dict[key_dset][key_loader]['no_DA_pred_labels'] = np.argmax(tmp_yy_pred_P, axis=1)\n",
    "\n",
    "# # compute the validation set results for the networks trained with domain adaptation (employing the JPAS spectra)\n",
    "# key_dset = \"val\"\n",
    "# key_loader = \"JPAS_matched\"\n",
    "\n",
    "# tmp_xx = xx_dict[key_dset][key_loader]\n",
    "# with torch.no_grad():\n",
    "#     tmp_features = model_encoder_DA(tmp_xx)\n",
    "#     tmp_logits = model_downstream_DA(tmp_features)\n",
    "# tmp_yy_pred_P = torch.nn.functional.softmax(tmp_logits, dim=1).cpu().numpy()\n",
    "\n",
    "# yy_dict[key_dset][key_loader]['DA_features'] = tmp_features.cpu().numpy()\n",
    "# yy_dict[key_dset][key_loader]['DA_pred_Probabilities'] = tmp_yy_pred_P\n",
    "# yy_dict[key_dset][key_loader]['DA_pred_labels'] = np.argmax(tmp_yy_pred_P, axis=1)\n",
    "\n",
    "# compute the test set results for the networks trained without domain adaptation (employing the JPAS spectra)\n",
    "key_dset = \"test\"\n",
    "key_loader = \"JPAS_matched\"\n",
    "\n",
    "tmp_xx = xx_dict[key_dset][key_loader]\n",
    "with torch.no_grad():\n",
    "    tmp_features = model_encoder_no_DA(tmp_xx)\n",
    "    tmp_logits = model_downstream_no_DA(tmp_features)\n",
    "tmp_yy_pred_P = torch.nn.functional.softmax(tmp_logits, dim=1).cpu().numpy()\n",
    "\n",
    "yy_dict[key_dset][key_loader]['no_DA_features'] = tmp_features.cpu().numpy()\n",
    "yy_dict[key_dset][key_loader]['no_DA_pred_Probabilities'] = tmp_yy_pred_P\n",
    "yy_dict[key_dset][key_loader]['no_DA_pred_labels'] = np.argmax(tmp_yy_pred_P, axis=1)\n",
    "\n",
    "# compute the test set results for the networks trained with domain adaptation (employing the JPAS spectra)\n",
    "key_dset = \"test\"\n",
    "key_loader = \"JPAS_matched\"\n",
    "\n",
    "tmp_xx = xx_dict[key_dset][key_loader]\n",
    "with torch.no_grad():\n",
    "    tmp_features = model_encoder_DA(tmp_xx)\n",
    "    tmp_logits = model_downstream_DA(tmp_features)\n",
    "tmp_yy_pred_P = torch.nn.functional.softmax(tmp_logits, dim=1).cpu().numpy()\n",
    "\n",
    "yy_dict[key_dset][key_loader]['DA_features'] = tmp_features.cpu().numpy()\n",
    "yy_dict[key_dset][key_loader]['DA_pred_Probabilities'] = tmp_yy_pred_P\n",
    "yy_dict[key_dset][key_loader]['DA_pred_labels'] = np.argmax(tmp_yy_pred_P, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_dset = \"val\"\n",
    "key_loader = \"DESI_only\"\n",
    "yy_true_no_DA_val = yy_dict[key_dset][key_loader]['SPECTYPE_int']\n",
    "yy_pred_P_no_DA_val = yy_dict[key_dset][key_loader]['no_DA_pred_Probabilities']\n",
    "yy_pred_no_DA_val = yy_dict[key_dset][key_loader]['no_DA_pred_labels']\n",
    "confusion_matrix = evaluation_tools.plot_confusion_matrix(\n",
    "    yy_true_no_DA_val, yy_pred_P_no_DA_val,\n",
    "    class_names=global_setup.class_names,\n",
    "    cmap=plt.cm.RdYlGn, title=\"Validation no DA\"\n",
    ")\n",
    "\n",
    "# key_dset = \"val\"\n",
    "# key_loader = \"JPAS_matched\"\n",
    "# yy_true_DA_val = yy_dict[key_dset][key_loader]['SPECTYPE_int']\n",
    "# yy_pred_P_DA_val = yy_dict[key_dset][key_loader]['DA_pred_Probabilities']\n",
    "# yy_pred_DA_val = yy_dict[key_dset][key_loader]['DA_pred_labels']\n",
    "# confusion_matrix = evaluation_tools.plot_confusion_matrix(\n",
    "#     yy_true_DA_val, yy_pred_P_DA_val,\n",
    "#     class_names=global_setup.class_names,\n",
    "#     cmap=plt.cm.RdYlGn, title=\"Validation DA\"\n",
    "# )\n",
    "\n",
    "key_dset = \"test\"\n",
    "key_loader = \"JPAS_matched\"\n",
    "yy_true_no_DA_test = yy_dict[key_dset][key_loader]['SPECTYPE_int']\n",
    "yy_pred_P_no_DA_test = yy_dict[key_dset][key_loader]['no_DA_pred_Probabilities']\n",
    "yy_pred_no_DA_test = yy_dict[key_dset][key_loader]['no_DA_pred_labels']\n",
    "confusion_matrix = evaluation_tools.plot_confusion_matrix(\n",
    "    yy_true_no_DA_test, yy_pred_P_no_DA_test,\n",
    "    class_names=global_setup.class_names,\n",
    "    cmap=plt.cm.RdYlGn, title=\"Test no DA\"\n",
    ")\n",
    "\n",
    "key_dset = \"test\"\n",
    "key_loader = \"JPAS_matched\"\n",
    "yy_true_DA_test = yy_dict[key_dset][key_loader]['SPECTYPE_int']\n",
    "yy_pred_P_DA_test = yy_dict[key_dset][key_loader]['DA_pred_Probabilities']\n",
    "yy_pred_DA_test = yy_dict[key_dset][key_loader]['DA_pred_labels']\n",
    "confusion_matrix = evaluation_tools.plot_confusion_matrix(\n",
    "    yy_true_DA_test, yy_pred_P_DA_test,\n",
    "    class_names=global_setup.class_names,\n",
    "    cmap=plt.cm.RdYlGn, title=\"Test DA\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_tools.compare_TPR_confusion_matrices(\n",
    "    yy_true_no_DA_val,\n",
    "    yy_pred_P_no_DA_val,\n",
    "    yy_true_no_DA_test,\n",
    "    yy_pred_P_no_DA_test,\n",
    "    class_names=global_setup.class_names,\n",
    "    figsize=(10, 7),\n",
    "    cmap='seismic',\n",
    "    title='Performance lost no DA -- Val. (DESI-mocks) VS test (JPAS spectra)',\n",
    "    name_1 = \"Val. Mock\",\n",
    "    name_2 = \"Test JPAS\"\n",
    ")\n",
    "metrics = evaluation_tools.compare_sets_performance(\n",
    "    yy_true_no_DA_val, yy_pred_P_no_DA_val,\n",
    "    yy_true_no_DA_test, yy_pred_P_no_DA_test,\n",
    "    class_names=global_setup.class_names,\n",
    "    name_1=\"Val. Mock\",\n",
    "    name_2=\"Test JPAS\"\n",
    ")\n",
    "\n",
    "# evaluation_tools.compare_TPR_confusion_matrices(\n",
    "#     yy_true_DA_val,\n",
    "#     yy_pred_P_DA_val,\n",
    "#     yy_true_DA_test,\n",
    "#     yy_pred_P_DA_test,\n",
    "#     class_names=global_setup.class_names,\n",
    "#     figsize=(10, 7),\n",
    "#     cmap='seismic',\n",
    "#     title='Performance lost with DA -- Val. (JPAS spectra) VS test (JPAS spectra)',\n",
    "#     name_1 = \"Val. JPAS\",\n",
    "#     name_2 = \"Test JPAS\"\n",
    "# )\n",
    "# metrics = evaluation_tools.compare_sets_performance(\n",
    "#     yy_true_DA_val, yy_pred_P_DA_val,\n",
    "#     yy_true_DA_test, yy_pred_P_DA_test,\n",
    "#     class_names=global_setup.class_names,\n",
    "#     name_1=\"Val. JPAS\",\n",
    "#     name_2=\"Test JPAS\"\n",
    "# )\n",
    "\n",
    "evaluation_tools.compare_TPR_confusion_matrices(\n",
    "    yy_true_no_DA_test,\n",
    "    yy_pred_P_no_DA_test,\n",
    "    yy_true_DA_test,\n",
    "    yy_pred_P_DA_test,\n",
    "    class_names=global_setup.class_names,\n",
    "    figsize=(10, 7),\n",
    "    cmap='seismic',\n",
    "    title='Performance comparison no DA VES with DA (Test sets JPAS spectra)',\n",
    "    name_1 = \"No DA\",\n",
    "    name_2 = \"With DA\"\n",
    ")\n",
    "metrics = evaluation_tools.compare_sets_performance(\n",
    "    yy_true_no_DA_test, yy_pred_P_no_DA_test,\n",
    "    yy_true_DA_test, yy_pred_P_DA_test,\n",
    "    class_names=global_setup.class_names,\n",
    "    name_1=\"No DA\",\n",
    "    name_2=\"With DA\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare magnitudes for combinations of interest\n",
    "key_pairs = [(\"val\", \"JPAS_matched\"), (\"test\", \"JPAS_matched\"), (\"val\", \"DESI_only\"), (\"test\", \"DESI_only\")]\n",
    "mag_dict = {}\n",
    "for key_dset, key_loader in key_pairs:\n",
    "    flux_R = yy_dict[key_dset][key_loader]['DESI_FLUX_R']\n",
    "    magnitude_R = np.full_like(flux_R, np.nan)\n",
    "    valid_flux = flux_R > 0\n",
    "    magnitude_R[valid_flux] = 22.5 - 2.5 * np.log10(flux_R[valid_flux])\n",
    "    mag_dict[(key_dset, key_loader)] = magnitude_R\n",
    "\n",
    "# Compute global range from all sets\n",
    "all_mags = np.concatenate([v[np.isfinite(v)] for v in mag_dict.values()])\n",
    "min_mag, max_mag = np.nanmin(all_mags), np.nanmax(all_mags)\n",
    "ranges = [(min_mag, 17), (17, 19), (19, 21), (21, 22), (22, 22.5), (22.5, max_mag)]\n",
    "colors = ['k', 'purple', 'blue', 'green', 'orange', 'red']\n",
    "\n",
    "masks_all = plotting_utils.plot_histogram_with_ranges_multiple(\n",
    "    mag_dict, ranges=ranges, colors=colors, bins=42,\n",
    "    x_label=\"DESI Magnitude (R)\",\n",
    "    title=\"DESI R-band Magnitudes by Dataset Split and Loader\"\n",
    ")\n",
    "\n",
    "# massage masks_all to a dictionary with mask like bin indices\n",
    "bin_index_dict = {}\n",
    "for key in masks_all.keys():\n",
    "    n_samples = len(next(iter(masks_all[key].values())))  # length from first mask\n",
    "    bin_indices = np.full(n_samples, -1, dtype=int)  # default: -1 means \"unassigned\"\n",
    "    for bin_id, mag_range in enumerate(ranges):\n",
    "        mask = masks_all[key][mag_range]\n",
    "        bin_indices[mask] = bin_id\n",
    "    bin_index_dict[key] = bin_indices\n",
    "\n",
    "# include this mask as a new feature in the yy_dicts\n",
    "for key in bin_index_dict:\n",
    "    key_dset, key_loader = key\n",
    "    yy_dict[key_dset][key_loader]['MAG_BIN_ID'] = bin_index_dict[key]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
