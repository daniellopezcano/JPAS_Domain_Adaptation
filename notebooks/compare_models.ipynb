{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s') # NOTSET, DEBUG, INFO, WARN, ERROR, CRITICAL\n",
    "\n",
    "from JPAS_DA import global_setup\n",
    "\n",
    "from JPAS_DA.data import loading_tools\n",
    "from JPAS_DA.data import cleaning_tools\n",
    "from JPAS_DA.data import crossmatch_tools\n",
    "from JPAS_DA.data import process_dset_splits\n",
    "from JPAS_DA.data import wrapper_data_loaders\n",
    "\n",
    "from JPAS_DA.models import model_building_tools\n",
    "from JPAS_DA.training import save_load_tools\n",
    "from JPAS_DA.evaluation import evaluation_tools\n",
    "from JPAS_DA.wrapper_wandb import wrapper_tools\n",
    "from JPAS_DA.evaluation import evaluation_tools\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "import fitsio\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "from JPAS_DA.utils import plotting_utils\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('default')\n",
    "plt.close('all')\n",
    "font, rcnew = plotting_utils.matplotlib_default_config()\n",
    "mpl.rc('font', **font)\n",
    "plt.rcParams.update(rcnew)\n",
    "plt.style.use('tableau-colorblind10')\n",
    "%matplotlib widget\n",
    "\n",
    "from JPAS_DA.utils import aux_tools\n",
    "aux_tools.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_load_no_DA = \"09_no_DA\"\n",
    "path_load_DA = \"09_DA\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, model_encoder_no_DA = save_load_tools.load_model_from_checkpoint(os.path.join(global_setup.path_models, path_load_no_DA, \"model_encoder.pt\"), model_building_tools.create_mlp)\n",
    "_, model_downstream_no_DA = save_load_tools.load_model_from_checkpoint(os.path.join(global_setup.path_models, path_load_no_DA, \"model_downstream.pt\"), model_building_tools.create_mlp)\n",
    "\n",
    "_, model_encoder_DA = save_load_tools.load_model_from_checkpoint(os.path.join(global_setup.path_models, path_load_DA, \"model_encoder.pt\"), model_building_tools.create_mlp)\n",
    "_, model_downstream_DA = save_load_tools.load_model_from_checkpoint(os.path.join(global_setup.path_models, path_load_DA, \"model_downstream.pt\"), model_building_tools.create_mlp)\n",
    "\n",
    "_ = evaluation_tools.compare_model_parameters(model_downstream_no_DA, model_downstream_DA, rtol=1e-2, atol=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, config_no_DA = wrapper_tools.load_and_massage_config_file(os.path.join(global_setup.path_models, path_load_no_DA, \"config.yaml\"), path_load_no_DA)\n",
    "_, config_DA = wrapper_tools.load_and_massage_config_file(os.path.join(global_setup.path_models, path_load_DA, \"config.yaml\"), path_load_DA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_data = config_DA[\"data\"]\n",
    "\n",
    "tmp_key = \"data_paths\"\n",
    "root_path = config_data[tmp_key][\"root_path\"]\n",
    "load_JPAS_data = config_data[tmp_key][\"load_JPAS_data\"]\n",
    "load_DESI_data = config_data[tmp_key][\"load_DESI_data\"]\n",
    "random_seed_load = config_data[tmp_key][\"random_seed_load\"]\n",
    "\n",
    "tmp_key = \"dict_clean_data_options\"\n",
    "apply_masks = config_data[tmp_key][\"apply_masks\"]\n",
    "mask_indices = config_data[tmp_key][\"mask_indices\"]\n",
    "magic_numbers = config_data[tmp_key][\"magic_numbers\"]\n",
    "i_band_sn_threshold = config_data[tmp_key][\"i_band_sn_threshold\"]\n",
    "magnitude_flux_key = config_data[tmp_key][\"magnitude_flux_key\"]\n",
    "magnitude_threshold = config_data[tmp_key][\"magnitude_threshold\"]\n",
    "z_lim_QSO_cut = config_data[tmp_key][\"z_lim_QSO_cut\"]\n",
    "\n",
    "tmp_key = \"dict_split_data_options\"\n",
    "train_ratio_both = config_data[tmp_key][\"train_ratio_both\"]\n",
    "val_ratio_both = config_data[tmp_key][\"val_ratio_both\"]\n",
    "test_ratio_both = config_data[tmp_key][\"test_ratio_both\"]\n",
    "random_seed_split_both = config_data[tmp_key][\"random_seed_split_both\"]\n",
    "train_ratio_only_DESI = config_data[tmp_key][\"train_ratio_only_DESI\"]\n",
    "val_ratio_only_DESI = config_data[tmp_key][\"val_ratio_only_DESI\"]\n",
    "test_ratio_only_DESI = config_data[tmp_key][\"test_ratio_only_DESI\"]\n",
    "random_seed_split_only_DESI = config_data[tmp_key][\"random_seed_split_only_DESI\"]\n",
    "\n",
    "define_dataset_loaders_keys = ['DESI_only', \"JPAS_matched\"]\n",
    "keys_xx = config_data[\"features_labels_options\"][\"keys_xx\"]\n",
    "keys_yy = [\"SPECTYPE_int\", \"TARGETID\", \"DESI_FLUX_R\"]\n",
    "normalize = True\n",
    "provided_normalization = config_data[\"provided_normalization\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ───────────────────────────────────────────────────── #\n",
    "# 1. Load raw JPAS and DESI datasets\n",
    "# ───────────────────────────────────────────────────── #\n",
    "logging.info(\"\\n\\n1️⃣: Loading datasets from disk...\")\n",
    "DATA = loading_tools.load_dsets(\n",
    "    root_path=root_path,\n",
    "    datasets_jpas=load_JPAS_data,\n",
    "    datasets_desi=load_DESI_data,\n",
    "    random_seed=random_seed_load\n",
    ")\n",
    "\n",
    "# ───────────────────────────────────────────────────── #\n",
    "# 2. Apply cleaning and masking procedures\n",
    "# ───────────────────────────────────────────────────── #\n",
    "logging.info(\"\\n\\n2️⃣: Cleaning and masking data...\")\n",
    "DATA = cleaning_tools.clean_and_mask_data(\n",
    "    DATA=DATA,\n",
    "    apply_masks=apply_masks,\n",
    "    mask_indices=mask_indices,\n",
    "    magic_numbers=magic_numbers,\n",
    "    i_band_sn_threshold=i_band_sn_threshold,\n",
    "    magnitude_flux_key=magnitude_flux_key,\n",
    "    magnitude_threshold=magnitude_threshold,\n",
    "    z_lim_QSO_cut=z_lim_QSO_cut\n",
    ")\n",
    "\n",
    "# ───────────────────────────────────────────────────── #\n",
    "# 3. Crossmatch JPAS and DESI using TARGETID\n",
    "# ───────────────────────────────────────────────────── #\n",
    "logging.info(\"\\n\\n3️⃣: Crossmatching JPAS and DESI TARGETIDs...\")\n",
    "Dict_LoA = {\"both\": {}, \"only\": {}}\n",
    "IDs_only_DESI, IDs_only_JPAS, IDs_both, \\\n",
    "Dict_LoA[\"only\"][\"DESI\"], Dict_LoA[\"only\"][\"JPAS\"], \\\n",
    "Dict_LoA[\"both\"][\"DESI\"], Dict_LoA[\"both\"][\"JPAS\"] = crossmatch_tools.crossmatch_IDs_two_datasets(\n",
    "    DATA[\"DESI\"]['TARGETID'], DATA[\"JPAS\"]['TARGETID']\n",
    ")\n",
    "\n",
    "# ───────────────────────────────────────────────────── #\n",
    "# 4. Perform train/val/test splits\n",
    "# ───────────────────────────────────────────────────── #\n",
    "logging.info(\"\\n\\n4️⃣: Splitting data into train/val/test...\")\n",
    "Dict_LoA_split = {\"both\": {}, \"only\": {}}\n",
    "\n",
    "Dict_LoA_split[\"both\"][\"JPAS\"] = process_dset_splits.split_LoA(\n",
    "    Dict_LoA[\"both\"][\"JPAS\"], train_ratio_both, val_ratio_both, test_ratio_both, seed=random_seed_split_both\n",
    ")\n",
    "Dict_LoA_split[\"both\"][\"DESI\"] = process_dset_splits.split_LoA(\n",
    "    Dict_LoA[\"both\"][\"DESI\"], train_ratio_both, val_ratio_both, test_ratio_both, seed=random_seed_split_both\n",
    ")\n",
    "Dict_LoA_split[\"only\"][\"DESI\"] = process_dset_splits.split_LoA(\n",
    "    Dict_LoA[\"only\"][\"DESI\"], train_ratio_only_DESI, val_ratio_only_DESI, test_ratio_only_DESI, seed=random_seed_split_only_DESI\n",
    ")\n",
    "\n",
    "# ───────────────────────────────────────────────────── #\n",
    "# 5. Load data\n",
    "# ───────────────────────────────────────────────────── #\n",
    "logging.info(\"\\n\\n5️⃣: Load and normalize data...\")\n",
    "\n",
    "xx_dict = {}\n",
    "yy_dict = {}\n",
    "for key_dset in [\"val\", \"test\"]:\n",
    "    xx_dict[key_dset] = {}\n",
    "    yy_dict[key_dset] = {}\n",
    "    logging.info(f\"⚙️ Preparing split: {key_dset}\")\n",
    "    for key_loader in define_dataset_loaders_keys:\n",
    "        logging.info(f\"├── {key_loader}\")\n",
    "        if key_loader == \"DESI_combined\":\n",
    "            LoA, xx, yy = process_dset_splits.extract_and_combine_DESI_data(\n",
    "                Dict_LoA_split[\"only\"][\"DESI\"][key_dset], Dict_LoA_split[\"both\"][\"DESI\"][key_dset], DATA[\"DESI\"], keys_xx, keys_yy\n",
    "            )\n",
    "        elif key_loader == \"DESI_only\":\n",
    "            LoA, xx, yy = process_dset_splits.extract_data_using_LoA(\n",
    "                Dict_LoA_split[\"only\"][\"DESI\"][key_dset], DATA[\"DESI\"], keys_xx, keys_yy\n",
    "            )\n",
    "        elif key_loader == \"DESI_matched\":\n",
    "            LoA, xx, yy = process_dset_splits.extract_data_using_LoA(\n",
    "                Dict_LoA_split[\"both\"][\"DESI\"][key_dset], DATA[\"DESI\"], keys_xx, keys_yy\n",
    "            )\n",
    "        elif key_loader == \"JPAS_matched\":\n",
    "            LoA, xx, yy = process_dset_splits.extract_data_using_LoA(\n",
    "                Dict_LoA_split[\"both\"][\"JPAS\"][key_dset], DATA[\"JPAS\"], keys_xx, keys_yy\n",
    "            )\n",
    "        # Normalize, reshape, and stack all features in one pass\n",
    "        xx_stacked = np.concatenate([\n",
    "            np.atleast_2d((xx[kk] - provided_normalization[0][ii]) / provided_normalization[1][ii]).reshape(xx[kk].shape[0], -1)\n",
    "            for ii, kk in enumerate(xx)\n",
    "        ], axis=1)\n",
    "\n",
    "        # Store as torch tensor\n",
    "        xx_dict[key_dset][key_loader] = torch.tensor(xx_stacked, dtype=torch.float32, device=\"cpu\")\n",
    "        yy_dict[key_dset][key_loader] = yy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the validation set results for the networks trained without domain adaptation (employing the DESI mock spectra)\n",
    "key_dset = \"val\"\n",
    "key_loader = \"DESI_only\"\n",
    "tmp_xx = xx_dict[key_dset][key_loader]\n",
    "with torch.no_grad():\n",
    "    tmp_features = model_encoder_no_DA(tmp_xx)\n",
    "    tmp_logits = model_downstream_no_DA(tmp_features)\n",
    "tmp_yy_pred_P = torch.nn.functional.softmax(tmp_logits, dim=1).cpu().numpy()\n",
    "yy_dict[key_dset][key_loader]['no_DA_features'] = tmp_features.cpu().numpy()\n",
    "yy_dict[key_dset][key_loader]['no_DA_pred_Probabilities'] = tmp_yy_pred_P\n",
    "yy_dict[key_dset][key_loader]['no_DA_pred_labels'] = np.argmax(tmp_yy_pred_P, axis=1)\n",
    "yy_true_no_DA_val = yy_dict[key_dset][key_loader]['SPECTYPE_int']\n",
    "yy_pred_P_no_DA_val = yy_dict[key_dset][key_loader]['no_DA_pred_Probabilities']\n",
    "yy_pred_no_DA_val = yy_dict[key_dset][key_loader]['no_DA_pred_labels']\n",
    "\n",
    "# compute the test set results for the networks trained without domain adaptation (employing the JPAS spectra)\n",
    "key_dset = \"test\"\n",
    "key_loader = \"JPAS_matched\"\n",
    "tmp_xx = xx_dict[key_dset][key_loader]\n",
    "with torch.no_grad():\n",
    "    tmp_features = model_encoder_no_DA(tmp_xx)\n",
    "    tmp_logits = model_downstream_no_DA(tmp_features)\n",
    "tmp_yy_pred_P = torch.nn.functional.softmax(tmp_logits, dim=1).cpu().numpy()\n",
    "yy_dict[key_dset][key_loader]['no_DA_features'] = tmp_features.cpu().numpy()\n",
    "yy_dict[key_dset][key_loader]['no_DA_pred_Probabilities'] = tmp_yy_pred_P\n",
    "yy_dict[key_dset][key_loader]['no_DA_pred_labels'] = np.argmax(tmp_yy_pred_P, axis=1)\n",
    "yy_true_no_DA_test = yy_dict[key_dset][key_loader]['SPECTYPE_int']\n",
    "yy_pred_P_no_DA_test = yy_dict[key_dset][key_loader]['no_DA_pred_Probabilities']\n",
    "yy_pred_no_DA_test = yy_dict[key_dset][key_loader]['no_DA_pred_labels']\n",
    "\n",
    "# compute the validation set results for the networks trained with domain adaptation (employing the JPAS spectra)\n",
    "key_dset = \"val\"\n",
    "key_loader = \"JPAS_matched\"\n",
    "tmp_xx = xx_dict[key_dset][key_loader]\n",
    "with torch.no_grad():\n",
    "    tmp_features = model_encoder_DA(tmp_xx)\n",
    "    tmp_logits = model_downstream_DA(tmp_features)\n",
    "tmp_yy_pred_P = torch.nn.functional.softmax(tmp_logits, dim=1).cpu().numpy()\n",
    "yy_dict[key_dset][key_loader]['DA_features'] = tmp_features.cpu().numpy()\n",
    "yy_dict[key_dset][key_loader]['DA_pred_Probabilities'] = tmp_yy_pred_P\n",
    "yy_dict[key_dset][key_loader]['DA_pred_labels'] = np.argmax(tmp_yy_pred_P, axis=1)\n",
    "yy_true_DA_val = yy_dict[key_dset][key_loader]['SPECTYPE_int']\n",
    "yy_pred_P_DA_val = yy_dict[key_dset][key_loader]['DA_pred_Probabilities']\n",
    "yy_pred_DA_val = yy_dict[key_dset][key_loader]['DA_pred_labels']\n",
    "\n",
    "# compute the test set results for the networks trained with domain adaptation (employing the JPAS spectra)\n",
    "key_dset = \"test\"\n",
    "key_loader = \"JPAS_matched\"\n",
    "tmp_xx = xx_dict[key_dset][key_loader]\n",
    "with torch.no_grad():\n",
    "    tmp_features = model_encoder_DA(tmp_xx)\n",
    "    tmp_logits = model_downstream_DA(tmp_features)\n",
    "tmp_yy_pred_P = torch.nn.functional.softmax(tmp_logits, dim=1).cpu().numpy()\n",
    "yy_dict[key_dset][key_loader]['DA_features'] = tmp_features.cpu().numpy()\n",
    "yy_dict[key_dset][key_loader]['DA_pred_Probabilities'] = tmp_yy_pred_P\n",
    "yy_dict[key_dset][key_loader]['DA_pred_labels'] = np.argmax(tmp_yy_pred_P, axis=1)\n",
    "yy_true_DA_test = yy_dict[key_dset][key_loader]['SPECTYPE_int']\n",
    "yy_pred_P_DA_test = yy_dict[key_dset][key_loader]['DA_pred_Probabilities']\n",
    "yy_pred_DA_test = yy_dict[key_dset][key_loader]['DA_pred_labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the confusion matrix validation results for the networks trained without domain adaptation (employing the Validation DESI mock spectra)\n",
    "confusion_matrix = evaluation_tools.plot_confusion_matrix(\n",
    "    yy_true_no_DA_val, yy_pred_P_no_DA_val, class_names=global_setup.class_names, cmap=plt.cm.RdYlGn, title=\"Validation no-DA\"\n",
    ")\n",
    "# plot the confusion matrix test results for the networks trained without domain adaptation (employing the Test JPAS spectra)\n",
    "confusion_matrix = evaluation_tools.plot_confusion_matrix(\n",
    "    yy_true_no_DA_test, yy_pred_P_no_DA_test, class_names=global_setup.class_names, cmap=plt.cm.RdYlGn, title=\"Test no-DA\"\n",
    ")\n",
    "# plot the confusion matrix test results for the networks trained with domain adaptation (employing the Test JPAS spectra)\n",
    "confusion_matrix = evaluation_tools.plot_confusion_matrix(\n",
    "    yy_true_DA_test, yy_pred_P_DA_test, class_names=global_setup.class_names, cmap=plt.cm.RdYlGn, title=\"Test DA\"\n",
    ")\n",
    "# compare performance between the no-DA-validation (employing the Validation DESI mock spectra) and no-DA-test (employing the Test JPAS spectra)\n",
    "evaluation_tools.compare_TPR_confusion_matrices(\n",
    "    yy_true_no_DA_val, yy_pred_P_no_DA_val, yy_true_no_DA_test, yy_pred_P_no_DA_test,\n",
    "    class_names=global_setup.class_names, figsize=(10, 7), cmap='seismic',\n",
    "    title='Performance lost no-DA -- Validation (mocks) VS Test (JPAS)', name_1 = \"Val. Mock\", name_2 = \"Test JPAS\"\n",
    ")\n",
    "metrics, F1_1, F1_2 = evaluation_tools.compare_sets_performance(\n",
    "    yy_true_no_DA_val, yy_pred_P_no_DA_val, yy_true_no_DA_test, yy_pred_P_no_DA_test,\n",
    "    class_names=global_setup.class_names, name_1=\"Val. Mock\", name_2=\"Test JPAS\"\n",
    ")\n",
    "# compare performance between the no-DA-test (employing the Test JPAS spectra) and DA-test (employing the Test JPAS spectra)\n",
    "evaluation_tools.compare_TPR_confusion_matrices(\n",
    "    yy_true_no_DA_test, yy_pred_P_no_DA_test, yy_true_DA_test, yy_pred_P_DA_test,\n",
    "    class_names=global_setup.class_names, figsize=(10, 7), cmap='seismic',\n",
    "    title='Performance no-DA VS DA (Tests JPAS spectra)', name_1 = \"No-DA\", name_2 = \"With DA\"\n",
    ")\n",
    "metrics, F1_1, F1_2 = evaluation_tools.compare_sets_performance(\n",
    "    yy_true_no_DA_test, yy_pred_P_no_DA_test, yy_true_DA_test, yy_pred_P_DA_test,\n",
    "    class_names=global_setup.class_names, name_1=\"No-DA\", name_2=\"With DA\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "JPAS_Ignasi = fitsio.read(\"/home/dlopez/Documentos/0.profesional/Postdoc/USP/Projects/JPAS_Domain_Adaptation/DATA/jpas_idr_classification_xmatch_desi_dr1.fits.gz\")\n",
    "\n",
    "yy_true_Ignasi = np.array(JPAS_Ignasi['SPECTYPE'][JPAS_Ignasi[\"is_in_desi_dr1\"]])\n",
    "yy_true_Ignasi = list(np.array(yy_true_Ignasi).astype(np.str_))\n",
    "REDSHIFT = np.array(JPAS_Ignasi['z'][JPAS_Ignasi[\"is_in_desi_dr1\"]])\n",
    "# Split between High and Low redshift quasars\n",
    "z_lim_QSO_cut = 2.1\n",
    "for ii in range(len(yy_true_Ignasi)):\n",
    "    if yy_true_Ignasi[ii] == \"QSO\":\n",
    "        if REDSHIFT[ii] < z_lim_QSO_cut:\n",
    "            yy_true_Ignasi[ii] = \"QSO_low\"\n",
    "        else:\n",
    "            yy_true_Ignasi[ii] = \"QSO_high\"\n",
    "yy_true_Ignasi, class_mapping = cleaning_tools.encode_strings_to_integers(yy_true_Ignasi)\n",
    "\n",
    "classification_keys = {\n",
    "    \"TRANS\" : ['conf_gal_TRANS', 'conf_hqso_TRANS', 'conf_lqso_TRANS', 'conf_star_TRANS'],\n",
    "    \"CBM\"   : ['conf_gal_CBM', 'conf_hqso_CBM', 'conf_lqso_CBM', 'conf_star_CBM']\n",
    "}\n",
    "yy_pred_P = {}\n",
    "for ii, key in enumerate(classification_keys):\n",
    "    yy_pred_P[key] = []\n",
    "    for jj, key_type in enumerate(classification_keys[key]):\n",
    "        yy_pred_P[key].append(np.array(JPAS_Ignasi[key_type][JPAS_Ignasi[\"is_in_desi_dr1\"]]))\n",
    "    yy_pred_P[key] = np.array(yy_pred_P[key]).T\n",
    "\n",
    "IDs_only_1, IDs_only_2, IDs_both, idxs_only_1, idxs_only_2, idxs_both_1, idxs_both_2 = crossmatch_tools.crossmatch_IDs_two_datasets(\n",
    "    yy_dict[\"test\"][\"JPAS_matched\"]['TARGETID'],\n",
    "    np.array(JPAS_Ignasi[\"TARGETID\"][JPAS_Ignasi[\"is_in_desi_dr1\"]])\n",
    ")\n",
    "idxs_both_me = np.concatenate(idxs_both_1)\n",
    "idxs_both_Ignasi = np.concatenate(idxs_both_2)\n",
    "\n",
    "yy_true_Ignasi_crossmatch = yy_true_Ignasi[idxs_both_Ignasi]\n",
    "yy_pred_P_Ignasi_crossmatch_CBM = yy_pred_P[\"CBM\"][idxs_both_Ignasi]\n",
    "yy_pred_P_Ignasi_crossmatch_TRANS = yy_pred_P[\"TRANS\"][idxs_both_Ignasi]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix = evaluation_tools.plot_confusion_matrix(\n",
    "    yy_true_Ignasi_crossmatch, yy_pred_P_Ignasi_crossmatch_CBM,\n",
    "    class_names=global_setup.class_names, cmap=plt.cm.RdYlGn, title=\"CBM\"\n",
    ")\n",
    "\n",
    "confusion_matrix = evaluation_tools.plot_confusion_matrix(\n",
    "    yy_true_Ignasi_crossmatch, yy_pred_P_Ignasi_crossmatch_TRANS,\n",
    "    class_names=global_setup.class_names, cmap=plt.cm.RdYlGn, title=\"TRANS\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare magnitudes for combinations of interest\n",
    "key_pairs = [(\"test\", \"JPAS_matched\"), (\"val\", \"DESI_only\")]\n",
    "mag_dict = {}\n",
    "for key_dset, key_loader in key_pairs:\n",
    "    flux_R = yy_dict[key_dset][key_loader]['DESI_FLUX_R']\n",
    "    magnitude_R = np.full_like(flux_R, np.nan)\n",
    "    valid_flux = flux_R > 0\n",
    "    magnitude_R[valid_flux] = 22.5 - 2.5 * np.log10(flux_R[valid_flux])\n",
    "    mag_dict[(key_dset, key_loader)] = magnitude_R\n",
    "\n",
    "# Compute global range from all sets\n",
    "all_mags = np.concatenate([v[np.isfinite(v)] for v in mag_dict.values()])\n",
    "min_mag, max_mag = np.nanmin(all_mags), np.nanmax(all_mags)\n",
    "magnitude_ranges = [(17, 19), (19, 21), (21, 22), (22, 22.5)]\n",
    "colors = ['blue', 'green', 'orange', 'red']\n",
    "colormaps = [\n",
    "    plt.cm.Blues,\n",
    "    plt.cm.Greens,\n",
    "    plt.cm.YlOrBr,\n",
    "    plt.cm.Reds\n",
    "]\n",
    "\n",
    "masks_all = plotting_utils.plot_histogram_with_ranges_multiple(\n",
    "    mag_dict, ranges=magnitude_ranges, colors=colors, bins=42,\n",
    "    x_label=\"DESI Magnitude (R)\",\n",
    "    title=\"DESI R-band Magnitudes by Dataset Split and Loader\"\n",
    ")\n",
    "\n",
    "# massage masks_all to a dictionary with mask like bin indices\n",
    "bin_index_dict = {}\n",
    "for key in masks_all.keys():\n",
    "    n_samples = len(next(iter(masks_all[key].values())))  # length from first mask\n",
    "    bin_indices = np.full(n_samples, -1, dtype=int)  # default: -1 means \"unassigned\"\n",
    "    for bin_id, mag_range in enumerate(magnitude_ranges):\n",
    "        mask = masks_all[key][mag_range]\n",
    "        bin_indices[mask] = bin_id\n",
    "    bin_index_dict[key] = bin_indices\n",
    "\n",
    "# include this mask as a new feature in the yy_dicts\n",
    "for key in bin_index_dict:\n",
    "    key_dset, key_loader = key\n",
    "    yy_dict[key_dset][key_loader]['MAG_BIN_ID'] = bin_index_dict[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_min_Delta_F1 = -0.6\n",
    "y_max_Delta_F1 = 0.6\n",
    "\n",
    "# Storage for F1-scores per magnitude bin and comparison\n",
    "F1_scores_per_bin = {\n",
    "    \"Val. Mock.\": [],\n",
    "    \"DA\": [],\n",
    "    \"TRANS\": [],\n",
    "    \"CBM\": []\n",
    "}\n",
    "\n",
    "for ii in range(len(magnitude_ranges)):\n",
    "    mask_val_DESI = yy_dict[\"val\"][\"DESI_only\"]['MAG_BIN_ID'] == ii\n",
    "    mask_test_JPAS = yy_dict[\"test\"][\"JPAS_matched\"]['MAG_BIN_ID'] == ii\n",
    "\n",
    "    # plot the confusion matrix validation results for the networks trained without domain adaptation (employing the Validation DESI mock spectra)\n",
    "    confusion_matrix = evaluation_tools.plot_confusion_matrix(\n",
    "        yy_true_no_DA_val[mask_val_DESI], yy_pred_P_no_DA_val[mask_val_DESI],\n",
    "        class_names=global_setup.class_names, cmap=colormaps[ii],\n",
    "        title=\"Validation no-DA. Mag: (\" + str(magnitude_ranges[ii][0]) + \", \" + str(magnitude_ranges[ii][1]) + \"). #Obj.: \" + str(np.sum(mask_val_DESI))\n",
    "    )\n",
    "\n",
    "    # # plot the confusion matrix test results for the networks trained without domain adaptation (employing the Test JPAS spectra)\n",
    "    # confusion_matrix = evaluation_tools.plot_confusion_matrix(\n",
    "    #     yy_true_no_DA_test[mask_test_JPAS], yy_pred_P_no_DA_test[mask_test_JPAS],\n",
    "    #     class_names=global_setup.class_names, cmap=colormaps[ii],\n",
    "    #     title=\"Test no-DA. Mag: (\" + str(magnitude_ranges[ii][0]) + \", \" + str(magnitude_ranges[ii][1]) + \"). #Obj.: \" + str(np.sum(mask_test_JPAS))\n",
    "    # )\n",
    "\n",
    "    # plot the confusion matrix test results for the networks trained with domain adaptation (employing the Test JPAS spectra)\n",
    "    confusion_matrix = evaluation_tools.plot_confusion_matrix(\n",
    "        yy_true_DA_test[mask_test_JPAS], yy_pred_P_DA_test[mask_test_JPAS],\n",
    "        class_names=global_setup.class_names, cmap=colormaps[ii],\n",
    "        title=\"Test DA. Mag: (\" + str(magnitude_ranges[ii][0]) + \", \" + str(magnitude_ranges[ii][1]) + \"). #Obj.: \" + str(np.sum(mask_test_JPAS))\n",
    "    )\n",
    "\n",
    "    # compare performance between the no-DA-validation (employing the Validation DESI mock spectra) and no-DA-test (employing the Test JPAS spectra)\n",
    "    evaluation_tools.compare_TPR_confusion_matrices(\n",
    "        yy_true_no_DA_val[mask_val_DESI], yy_pred_P_no_DA_val[mask_val_DESI], yy_true_no_DA_test[mask_test_JPAS], yy_pred_P_no_DA_test[mask_test_JPAS],\n",
    "        class_names=global_setup.class_names, figsize=(10, 7), cmap='seismic',\n",
    "        title=\"Performance lost no-DA -- Validation (mocks) VS Test (JPAS). Mag: (\" + str(magnitude_ranges[ii][0]) + \", \" + str(magnitude_ranges[ii][1]) + \")\",\n",
    "        name_1 = \"Val. Mock\", name_2 = \"Test JPAS\"\n",
    "    )\n",
    "\n",
    "    metrics, F1_1, F1_2 = evaluation_tools.compare_sets_performance(\n",
    "        yy_true_no_DA_val[mask_val_DESI], yy_pred_P_no_DA_val[mask_val_DESI], yy_true_no_DA_test[mask_test_JPAS], yy_pred_P_no_DA_test[mask_test_JPAS],\n",
    "        class_names=global_setup.class_names, name_1=\"Val. Mock.\", name_2=\"Test JPAS\", plot_ROC_curves=False, y_min_Delta_F1=y_min_Delta_F1, y_max_Delta_F1=y_max_Delta_F1\n",
    "    )\n",
    "    F1_scores_per_bin[\"Val. Mock.\"].append(F1_1)\n",
    "\n",
    "    # compare performance between the no-DA-test (employing the Test JPAS spectra) and DA-test (employing the Test JPAS spectra)\n",
    "    evaluation_tools.compare_TPR_confusion_matrices(\n",
    "        yy_true_no_DA_test[mask_test_JPAS], yy_pred_P_no_DA_test[mask_test_JPAS], yy_true_DA_test[mask_test_JPAS], yy_pred_P_DA_test[mask_test_JPAS],\n",
    "        class_names=global_setup.class_names, figsize=(10, 7), cmap='seismic',\n",
    "        title=\"Performance no-DA VS DA (Tests JPAS spectra). Mag: (\" + str(magnitude_ranges[ii][0]) + \", \" + str(magnitude_ranges[ii][1]) + \")\",\n",
    "        name_1 = \"No-DA\", name_2 = \"With DA\"\n",
    "    )\n",
    "    \n",
    "    metrics, F1_1, F1_2 = evaluation_tools.compare_sets_performance(\n",
    "        yy_true_no_DA_test[mask_test_JPAS], yy_pred_P_no_DA_test[mask_test_JPAS], yy_true_DA_test[mask_test_JPAS], yy_pred_P_DA_test[mask_test_JPAS],\n",
    "        class_names=global_setup.class_names, name_1=\"No-DA\", name_2=\"With DA\", plot_ROC_curves=False, y_min_Delta_F1=y_min_Delta_F1, y_max_Delta_F1=y_max_Delta_F1\n",
    "    )\n",
    "    F1_scores_per_bin[\"DA\"].append(F1_2)\n",
    "\n",
    "    IDs_only_1, IDs_only_2, IDs_both, idxs_only_1, idxs_only_2, idxs_both_1, idxs_both_2 = crossmatch_tools.crossmatch_IDs_two_datasets(\n",
    "        yy_dict[\"test\"][\"JPAS_matched\"]['TARGETID'][mask_test_JPAS],\n",
    "        np.array(JPAS_Ignasi[\"TARGETID\"][JPAS_Ignasi[\"is_in_desi_dr1\"]])\n",
    "    )\n",
    "    idxs_both_Ignasi = np.concatenate(idxs_both_2)\n",
    "    yy_true_Ignasi_crossmatch = yy_true_Ignasi[idxs_both_Ignasi]\n",
    "    yy_pred_P_Ignasi_crossmatch_CBM = yy_pred_P[\"CBM\"][idxs_both_Ignasi]\n",
    "    yy_pred_P_Ignasi_crossmatch_TRANS = yy_pred_P[\"TRANS\"][idxs_both_Ignasi]\n",
    "\n",
    "    confusion_matrix = evaluation_tools.plot_confusion_matrix(\n",
    "        yy_true_Ignasi_crossmatch, yy_pred_P_Ignasi_crossmatch_CBM,\n",
    "        class_names=global_setup.class_names, cmap=colormaps[ii],\n",
    "        title=\"CBM. Mag: (\" + str(magnitude_ranges[ii][0]) + \", \" + str(magnitude_ranges[ii][1]) + \"). #Obj.: \" + str(len(yy_pred_P_Ignasi_crossmatch_CBM))\n",
    "    )\n",
    "\n",
    "    metrics, F1_1, F1_2 = evaluation_tools.compare_sets_performance(\n",
    "        yy_true_no_DA_val[mask_val_DESI], yy_pred_P_no_DA_val[mask_val_DESI], yy_true_Ignasi_crossmatch, yy_pred_P_Ignasi_crossmatch_CBM,\n",
    "        class_names=global_setup.class_names, name_1=\"Val. Mock.\", name_2=\"CBM\", plot_ROC_curves=False, y_min_Delta_F1=y_min_Delta_F1, y_max_Delta_F1=y_max_Delta_F1\n",
    "    )\n",
    "    F1_scores_per_bin[\"CBM\"].append(F1_2)\n",
    "\n",
    "    confusion_matrix = evaluation_tools.plot_confusion_matrix(\n",
    "        yy_true_Ignasi_crossmatch, yy_pred_P_Ignasi_crossmatch_TRANS,\n",
    "        class_names=global_setup.class_names, cmap=colormaps[ii],\n",
    "        title=\"TRANS. Mag: (\" + str(magnitude_ranges[ii][0]) + \", \" + str(magnitude_ranges[ii][1]) + \"). #Obj.: \" + str(len(yy_pred_P_Ignasi_crossmatch_TRANS))\n",
    "    )\n",
    "\n",
    "    metrics, F1_1, F1_2 = evaluation_tools.compare_sets_performance(\n",
    "        yy_true_no_DA_val[mask_val_DESI], yy_pred_P_no_DA_val[mask_val_DESI], yy_true_Ignasi_crossmatch, yy_pred_P_Ignasi_crossmatch_TRANS,\n",
    "        class_names=global_setup.class_names, name_1=\"Val. Mock.\", name_2=\"TRANS\", plot_ROC_curves=False, y_min_Delta_F1=y_min_Delta_F1, y_max_Delta_F1=y_max_Delta_F1\n",
    "    )\n",
    "    F1_scores_per_bin[\"TRANS\"].append(F1_2)\n",
    "\n",
    "\n",
    "# Convert lists to arrays\n",
    "F1_dict = {k: np.array(v) for k, v in F1_scores_per_bin.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.patches import Patch\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "# Define structure\n",
    "class_names = [\"GALAXY\", \"QSO_low\", \"QSO_high\", \"STAR\"]\n",
    "num_classes = len(class_names)\n",
    "magnitude_bins = [\"17–19\", \"19–21\", \"21–22\", \"22–22.5\"]\n",
    "models = [\"Val. Mock.\", \"DA\", \"TRANS\", \"CBM\"]\n",
    "\n",
    "angles = np.linspace(0, 2 * np.pi, num_classes, endpoint=False).tolist()\n",
    "angles += angles[:1]\n",
    "\n",
    "# Define styles\n",
    "model_colors = {\n",
    "    \"Val. Mock.\": \"royalblue\",\n",
    "    \"DA\": \"royalblue\",\n",
    "    \"TRANS\": \"crimson\",\n",
    "    \"CBM\": \"limegreen\"\n",
    "}\n",
    "model_styles = {\n",
    "    \"Val. Mock.\": \"dashed\",\n",
    "    \"DA\": \"solid\",\n",
    "    \"TRANS\": \"dotted\",\n",
    "    \"CBM\": \"dotted\"\n",
    "}\n",
    "\n",
    "# Generate dummy F1 scores\n",
    "np.random.seed(1)\n",
    "F1_dict = {\n",
    "    model: np.random.rand(4, num_classes) * 0.4 + 0.5 for model in models\n",
    "}\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10), subplot_kw=dict(polar=True))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, ax in enumerate(axes):\n",
    "    for model in models:\n",
    "        f1_vals = F1_dict[model][i].tolist()\n",
    "        f1_vals += f1_vals[:1]\n",
    "        ax.plot(angles, f1_vals, color=model_colors[model], linestyle=model_styles[model], linewidth=2)\n",
    "\n",
    "        macro_f1 = np.mean(F1_dict[model][i])\n",
    "        angle_pos = np.pi / 4 + (2*np.pi/num_classes)*(models.index(model))\n",
    "        r_pos = 1.2\n",
    "        ax.text(\n",
    "            angle_pos, r_pos,\n",
    "            f\"{model}\\nF1={macro_f1:.2f}\",\n",
    "            color=model_colors[model],\n",
    "            fontsize=9,\n",
    "            ha=\"center\", va=\"center\",\n",
    "            bbox=dict(facecolor='white', edgecolor=model_colors[model], boxstyle='round,pad=0.4', lw=1.5, ls=model_styles[model])\n",
    "        )\n",
    "\n",
    "    ax.set_title(f\"Magnitude Bin: {magnitude_bins[i]}\", fontsize=14, pad=15)\n",
    "    ax.set_theta_offset(np.pi / 2)\n",
    "    ax.set_theta_direction(-1)\n",
    "    ax.set_thetagrids(np.degrees(angles[:-1]), class_names, fontsize=10)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.tick_params(labelsize=8)  # Make radial ticks (F1) smaller here\n",
    "\n",
    "legend_lines = [Line2D([0], [0], color=model_colors[m], lw=2, linestyle=model_styles[m], label=m) for m in models]\n",
    "fig.legend(handles=legend_lines, loc=\"center right\", title=\"Model\", fontsize=14, title_fontsize=13)\n",
    "\n",
    "plt.suptitle(\"F1-score Radar Plot per Class for Each Magnitude Bin\", fontsize=16)\n",
    "plt.tight_layout(rect=[0, 0, 0.88, 0.95])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
